{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np\n",
    "import pprint\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from feature_engine.selection import DropDuplicateFeatures, DropConstantFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a list of the Cancer Data folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of folder names that will be used to look for the files\n",
    "folders = ['TCGA-ACC', 'TCGA-BLCA', 'TCGA-BRCA', 'TCGA-CESC', 'TCGA-CHOL', 'TCGA-COAD', 'TCGA-DLBC',\n",
    " 'TCGA-ESCA', 'TCGA-GBM', 'TCGA-HNSC', 'TCGA-KICH', 'TCGA-KIRC', 'TCGA-KIRP', 'TCGA-LAML', 'TCGA-LGG',\n",
    "  'TCGA-LIHC', 'TCGA-LUAD', 'TCGA-LUSC', 'TCGA-MESO', 'TCGA-OV', 'TCGA-PAAD', 'TCGA-PCPG', 'TCGA-PRAD',\n",
    "   'TCGA-READ', 'TCGA-SARC', 'TCGA-SKCM', 'TCGA-STAD', 'TCGA-TGCT', 'TCGA-THCA', 'TCGA-THYM', 'TCGA-UCEC',\n",
    "    'TCGA-UCS', 'TCGA-UVM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index values that will be used for each cancer type\n",
    "# change this to process a specific cancer type (0-32)\n",
    "Indx = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  miRNA data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from the subdirectories of [parent_directory]\n",
    "# The pattern is: *.mirna.tsv\n",
    "\n",
    "# Load the data for the first element of folders list\n",
    "mirna_list = glob.glob(\"[parent_directory]**/{}.mirna.tsv\".format(folders[Indx]), recursive=True) # change [parent_directory] to the parent directory where the data is stored\n",
    "\n",
    "# sort the list of files\n",
    "mirna_list.sort()\n",
    "# Load the file\n",
    "if len(mirna_list) > 0:\n",
    "    for file in mirna_list:\n",
    "        print(\"Loading file: \" + file)\n",
    "        df = pd.read_table(file)\n",
    "        print(df.shape)\n",
    "else:\n",
    "    print(\"No file matching the pattern found.\")\n",
    "print(len(mirna_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the first file and print the shape\n",
    "mirnafile1 = mirna_list[0]\n",
    "stemmirna = pd.read_table(mirnafile1)\n",
    "print(mirnafile1)\n",
    "print(stemmirna.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Transpose Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mirnastem=stemmirna.transpose()\n",
    "print(mirnastem.shape)\n",
    "mirnastem=mirnastem.reset_index()\n",
    "print(mirnastem.shape)\n",
    "mirnastem.columns = mirnastem.iloc[0]\n",
    "#remove first row from DataFrame\n",
    "mirnastem = mirnastem[1:]\n",
    "mirnastem=mirnastem.rename(columns={f'{mirnastem.columns[0]}':'sample'})\n",
    "mirnastem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Drop Constant Features (i.e. >99.8% similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine.selection import DropConstantFeatures\n",
    "sel1 = DropConstantFeatures(tol=0.998, variables=None, missing_values='raise')\n",
    "sel1.fit(mirnastem)\n",
    "mirnastem = sel1.transform(mirnastem)\n",
    "mirnastem.shape\n",
    "mirnastem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Remove Colinear Features (i.e. >80% correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temporary remove the first column for later adding it to the first column\n",
    "mirnastem1 = mirnastem.iloc[:, 1:]\n",
    "# check the variable format with pandas dtypes.\n",
    "print(mirnastem1.dtypes)\n",
    "# convert the variable to numerical variables\n",
    "mirnastem1 = mirnastem1.astype(float)\n",
    "# check the variable format with pandas dtypes.\n",
    "print(mirnastem1.dtypes)\n",
    "mirnastem1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove correlated features\n",
    "from feature_engine.selection import SmartCorrelatedSelection\n",
    "sel2 = SmartCorrelatedSelection(\n",
    "    variables=None,\n",
    "    method=\"pearson\",\n",
    "    threshold=0.8,\n",
    "    missing_values=\"raise\",\n",
    "    selection_method=\"variance\",\n",
    "    estimator=None,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel2.fit(mirnastem1)\n",
    "stemmirna = sel2.transform(mirnastem1)\n",
    "stemmirna.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the 'sample' column from mirnastem to the first column of stemmirna\n",
    "stemmirna.insert(0, 'sample', mirnastem['sample'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmirna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  DNA Methylation data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from the subdirectories of [parent_directory]\n",
    "# The pattern is: *.methylation450.tsv\n",
    "\n",
    "# Load the data\n",
    "meth_list = glob.glob(\"[parent_directory]/**/{}.methylation450.tsv\".format(folders[Indx]), recursive=True) # change [parent_directory] to the parent directory where the data is stored\n",
    "# sort the list of files\n",
    "meth_list.sort()\n",
    "# Load the file\n",
    "if len(meth_list) > 0:\n",
    "    for file in meth_list:\n",
    "        print(\"Loading file: \" + file)\n",
    "        df = pd.read_table(file)\n",
    "        print(df.shape)\n",
    "else:\n",
    "    print(\"No file matching the pattern found.\")\n",
    "print(len(meth_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the first file and print the shape\n",
    "meth1 = meth_list[0]\n",
    "dnamethylaton = pd.read_table(meth1)\n",
    "print(meth1)\n",
    "print(dnamethylaton.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Transpose Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnameth=dnamethylaton.transpose()\n",
    "print(dnameth.shape)\n",
    "dnameth=dnameth.reset_index()\n",
    "dnameth.columns=dnameth.iloc[0]\n",
    "dnameth=dnameth[1:]\n",
    "# rename the first column to 'sample'\n",
    "dnameth=dnameth.rename(columns={f'{dnameth.columns[0]}':'sample'})\n",
    "dnameth.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Remove NANs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove NaN values\n",
    "dnameth = dnameth.dropna(axis=1)\n",
    "dnameth.shape\n",
    "dnameth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Drop Constant Features (i.e. >95% similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine.selection import DropConstantFeatures\n",
    "sel1 = DropConstantFeatures(tol=0.95, variables=None, missing_values='raise')\n",
    "sel1.fit(dnameth)\n",
    "dnameth = sel1.transform(dnameth)\n",
    "dnameth.shape\n",
    "dnameth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Remove Colinear Features (i.e. >80% correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnameth=dnameth.dropna(axis=1)\n",
    "print(dnameth)\n",
    "dnamethyl=dnameth.drop(columns='sample')\n",
    "dnameth_columns = dnamethyl.columns.tolist()\n",
    "numerical_features = [col for col in dnameth_columns if dnameth[col].dtype == 'object']\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold \n",
    "# function that takes data and returns it after removing the features\n",
    "# having less than the given threshold variance\n",
    "\n",
    "def variance_threshold_selector(data, threshold=0.9):\n",
    "    selector = VarianceThreshold(threshold)\n",
    "    selector.fit(data)\n",
    "    return data[data.columns[selector.get_support(indices=True)]]\n",
    "\n",
    "hvdnameth = variance_threshold_selector(dnamethyl, 0.061) # change this value to see the effect, we have changed it so the reduced feature size is of similiar dimension.\n",
    "print(type(hvdnameth))\n",
    "print(hvdnameth.shape)\n",
    "samplenames=dnameth['sample']\n",
    "hvdnameth.insert(0,'sample',samplenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hvdnameth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Gene Expression data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from the subdirectories of [parent_directory]\n",
    "# The pattern is: *.gene-expr-RNAhtseq*.tsv\n",
    "\n",
    "# Load the data\n",
    "gene_list = glob.glob(\"[parent_directory]**/{}.gene-expr-RNAhtseq*.tsv\".format(folders[Indx]), recursive=True) # change [parent_directory] to the parent directory where the data is stored\n",
    "# sort the list of files\n",
    "gene_list.sort()\n",
    "# Load the file\n",
    "if len(gene_list) > 0:\n",
    "    for file in gene_list:\n",
    "        print(\"Loading file: \" + file)\n",
    "        df = pd.read_table(file)\n",
    "        print(df.shape)\n",
    "else:\n",
    "    print(\"No file matching the pattern found.\")\n",
    "print(len(gene_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the first file and print the shape\n",
    "gene_file1 = gene_list[0]\n",
    "genexpr = pd.read_table(gene_file1)\n",
    "print(gene_file1)\n",
    "print(genexpr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genexpr.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Transpose Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genexpress=genexpr.transpose()\n",
    "print(genexpress.shape)\n",
    "genexpress=genexpress.reset_index()\n",
    "print(genexpress.shape)\n",
    "genexpress.columns = genexpress.iloc[0]\n",
    "#remove first row from DataFrame\n",
    "genexpress = genexpress[1:]\n",
    "genexpress=genexpress.rename(columns={f'{genexpress.columns[0]}':'sample'})\n",
    "genexpress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Drop Constant Features (i.e. >95% similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine.selection import DropConstantFeatures\n",
    "sel1 = DropConstantFeatures(tol=0.95, variables=None, missing_values='raise')\n",
    "sel1.fit(genexpress)\n",
    "genexpress = sel1.transform(genexpress)\n",
    "genexpress.shape\n",
    "genexpress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Remove duplicate features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the selector\n",
    "sel2 = DropDuplicateFeatures(variables=None, missing_values='raise')\n",
    "# find the duplicate features, this might take a while\n",
    "sel2.fit(genexpress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's explore our list of duplicated features\n",
    "\n",
    "len(sel2.features_to_drop_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the duplicated features\n",
    "genexpress = sel2.transform(genexpress)\n",
    "genexpress.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Remove Colinear Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genexpress=genexpress.dropna(axis=1)\n",
    "print(genexpress)\n",
    "genes=genexpress.drop(columns='sample')\n",
    "genes_columns = genes.columns.tolist()\n",
    "numerical_features = [col for col in genes_columns if genexpress[col].dtype == 'object']\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold \n",
    "# function that takes data and returns it after removing the features\n",
    "# having less than the given threshold variance\n",
    "\n",
    "def variance_threshold_selector(data, threshold=0.9):\n",
    "    selector = VarianceThreshold(threshold)\n",
    "    selector.fit(data)\n",
    "    return data[data.columns[selector.get_support(indices=True)]]\n",
    "\n",
    "hvgenes = variance_threshold_selector(genes, 0.035)\n",
    "print(type(hvgenes))\n",
    "print(hvgenes.shape)\n",
    "samplenames=genexpress['sample']\n",
    "hvgenes.insert(0,'sample',samplenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hvgenes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Remove genes with expression < 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate column means except for the first column\n",
    "hvgenes_colms=hvgenes.iloc[:,1:]\n",
    "means=hvgenes_colms.mean()\n",
    "print(means)\n",
    "high=means[means>=10]\n",
    "print(high)\n",
    "print(high.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hvgenes_colms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hvgcolumns=hvgenes_colms.columns.tolist()\n",
    "print(hvgcolumns)\n",
    "print(len(hvgcolumns))\n",
    "print(len(hvgenes_colms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genestouse=[]\n",
    "for i in range(len(hvgcolumns)): \n",
    "    sc=hvgenes[f'{hvgcolumns[i]}']>7\n",
    "    b=sc.value_counts()\n",
    "    if len(b.index)==2:\n",
    "       genestouse.append(f'{hvgcolumns[i]}')\n",
    "print(genestouse)\n",
    "print(len(genestouse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genedataframe=hvgenes[genestouse]\n",
    "genedataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samplenames=hvgenes['sample']\n",
    "genedataframe.insert(0,'sample',samplenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genedataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Survival Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all files matching the pattern from the subdirectories of [parent_directory]\n",
    "# The pattern is: *.survival.tsv\n",
    "\n",
    "# surv_list = glob.glob(\"/home/80024223/data/Xena-GDC/**/*.survival.tsv\", recursive=True)\n",
    "surv_list = glob.glob(\" [parent_directory]**/{}.survival.tsv\".format(folders[Indx]), recursive=True) # change [parent_directory] to the parent directory where the data is stored\n",
    "# sort the list of files\n",
    "surv_list.sort()\n",
    "\n",
    "# Load the file\n",
    "if len(surv_list) > 0:\n",
    "    # print the filename and corresponding number of rows\n",
    "    for file in surv_list:\n",
    "        surv_df = pd.read_csv(file, sep='\\t')\n",
    "\n",
    "        print(f\"{file} & {surv_df.shape}\")\n",
    "else:\n",
    "    print(\"No file matching the pattern found.\")\n",
    "print(len(surv_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the first file and print the shape\n",
    "survfile1 = surv_list[0]\n",
    "surv = pd.read_table(survfile1)\n",
    "print(survfile1)\n",
    "print(surv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clinical Data (age, gender, race, cancer stage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index values that will be used for each cancer type\n",
    "# change this to process a specific cancer type (0-32)\n",
    "Indx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all files matching the pattern from the subdirectories of [parent_directory]\n",
    "# The pattern is: *.GDC_phenotype.tsv\n",
    "clin_list = glob.glob(\"[parent_directory]/{}.GDC_phenotype.tsv\".format(folders[Indx]), recursive=True) # change [parent_directory] to the parent directory where the data is stored\n",
    "\n",
    "# sort the list of files\n",
    "clin_list.sort()\n",
    "matching_columns = ['submitter_id.samples', 'age_at_index.demographic', 'age_at_diagnosis.diagnoses', 'gender.demographic', 'race.demographic', 'tumor_stage.diagnoses']\n",
    "# Load the file\n",
    "if len(clin_list) > 0:\n",
    "    for file in clin_list:\n",
    "        print(\"Loading file: \" + file)\n",
    "        df = pd.read_table(file)\n",
    "        df = df[matching_columns]\n",
    "else:\n",
    "    print(\"No file matching the pattern found.\")\n",
    "print(len(clin_list))\n",
    "# load the first file and print the shape\n",
    "clin1 = clin_list[0]\n",
    "clin = pd.read_table(clin1)\n",
    "clin = clin[matching_columns]\n",
    "print(clin.shape)\n",
    "clin.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Remove NANs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if columns have the NANs\n",
    "print(clin.isnull().sum())\n",
    "print(clin['age_at_index.demographic'].isnull().sum())\n",
    "print(clin['age_at_diagnosis.diagnoses'].isnull().sum())\n",
    "print(clin['gender.demographic'].isnull().sum())\n",
    "print(clin['race.demographic'].isnull().sum())\n",
    "print(clin['tumor_stage.diagnoses'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clin.shape)\n",
    "# drop the rows with NANs in clin['age_at_index.demographic']\n",
    "clin = clin.dropna(subset=['age_at_index.demographic'])\n",
    "print(clin.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if columns have the NANs\n",
    "print(clin.isnull().sum())\n",
    "print(clin['age_at_index.demographic'].isnull().sum())\n",
    "print(clin['age_at_diagnosis.diagnoses'].isnull().sum())\n",
    "print(clin['gender.demographic'].isnull().sum())\n",
    "print(clin['race.demographic'].isnull().sum())\n",
    "print(clin['tumor_stage.diagnoses'].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Convert categorical variables to Numerical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change Age columns to float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the 'age_at_index.demographic', 'age_at_diagnosis.diagnoses'column to float\n",
    "clin['age_at_index.demographic'] = clin['age_at_index.demographic'].astype(float)\n",
    "clin['age_at_diagnosis.diagnoses'] = clin['age_at_diagnosis.diagnoses'].astype(float)\n",
    "# check the variable format with pandas dtypes.\n",
    "print(clin.dtypes)\n",
    "# find the number of samples having age>50\n",
    "age50 = clin['age_at_index.demographic'] > 50\n",
    "print(age50.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gender: male=1, female=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the number of unique values in 'gender.demographic' column\n",
    "num_unique_genders = clin['gender.demographic'].nunique()\n",
    "unique_genders = clin['gender.demographic'].unique()\n",
    "# num_unique_genders = clin['gender.demographic'].nunique(dropna=False)\n",
    "print(f\"The 'gender.demographic' column has {num_unique_genders} unique values.\")\n",
    "print(f\"The unique values are: {unique_genders}\")\n",
    "# count the number of each unique value\n",
    "value_counts = clin['gender.demographic'].value_counts()\n",
    "print(value_counts)\n",
    "# convert the 'gender.demographic' to numerical values male=1, female=2\n",
    "gender_mapping = {'male': 1, 'female': 2}\n",
    "clin['gender'] = clin['gender.demographic'].map(gender_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Race: white=1, asian=2, black or african american=3, not reported=4, 'american indian or alaska native'=5, 'native hawaiian or other pacific islander'=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the number of unique values in clin['race.demographic' column\n",
    "num_unique_race = clin['race.demographic'].nunique()\n",
    "unique_race = clin['race.demographic'].unique()\n",
    "print(f\"The 'race.demographic' column has {num_unique_race} unique values.\")\n",
    "print(f\"The unique values in the 'race.demographic' column are: {unique_race}\")\n",
    "# count the number of each unique value\n",
    "value_counts = clin['race.demographic'].value_counts()\n",
    "print(value_counts)\n",
    "# convert the 'race.demographic' to numerical values white=1, asian=2, black or african american=3, not reported=4\n",
    "race_mapping = {'white': 1, 'asian': 2, 'black or african american': 3, 'not reported': 4, 'american indian or alaska native':5}\n",
    "clin['race'] = clin['race.demographic'].map(race_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cancer stage: stage 0=1, is=10, stage i=10, stage ia=11, stage ib=12, i/ii nos=20, stage ii=20, stage iia=21, stage iib=22, stage iic=23, stage iii=30, stage iiia=31, stage iiib=32, stage iiic=33, stage iv=40, stage iva=41, stage ivb=42, stage ivc=43, not reported=50, stage x=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the number of unique values in 'tumor_stage.diagnoses' column\n",
    "num_unique_tumors = clin['tumor_stage.diagnoses'].nunique()\n",
    "unique_tumors = clin['tumor_stage.diagnoses'].unique()\n",
    "print(f\"The 'tumor_stage.diagnoses' column has {num_unique_tumors} unique values.\")\n",
    "print(f\"The unique values are: {unique_tumors}\")\n",
    "# count the number of each unique value\n",
    "value_counts = clin['tumor_stage.diagnoses'].value_counts()\n",
    "print(value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the 'tumor_stage.diagnoses' to numerical values stage i=10, stage ii=20, stage iii=30, stage iv=40, not reported=50\n",
    "tumor_mapping = {'stage 0': 1, 'is': 10, 'stage i': 10, 'stage ia': 11, 'stage ib': 12, 'stage ic': 13, 'stage ii': 20, 'i/ii nos': 20, 'stage iia': 21, 'stage iib': 22, 'stage iic': 23, 'stage iii': 30, 'stage iiia': 31, 'stage iiib': 32, 'stage iiic': 33, 'stage iv': 40, 'stage iva': 41, 'stage ivb': 42, 'stage ivc': 43, 'not reported': 50, 'stage x': 50}\n",
    "clin['tumor_stage'] = clin['tumor_stage.diagnoses'].map(tumor_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the columns that are not needed\n",
    "clin.drop(['age_at_diagnosis.diagnoses', 'gender.demographic', 'race.demographic', 'tumor_stage.diagnoses'], axis=1, inplace=True)\n",
    "# rename the column age_at_diagnosis.demographic to age\n",
    "clin.rename(columns={'age_at_index.demographic': 'age'}, inplace=True)\n",
    "clin.rename(columns={'submitter_id.samples': 'sample'}, inplace=True)\n",
    "clin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clin.shape)\n",
    "# save the data to a csv file\n",
    "clin.to_csv(f\"{folders[Indx]}_clinical_data.csv\", index=False)\n",
    "print(f\"Data saved to {folders[Indx]}_clinical_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data from the csv file and confirm the shape\n",
    "clinical = pd.read_csv(f\"/home/80024223/data/Xena-GDC/preprocessed-data/{folders[Indx]}_clinical_data.csv\")\n",
    "print(clinical.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine the data modalities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### miRNA + Survival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the stemmirna and survival dataframes using the 'sample' column as the key and 'OS' and 'OS.time' as the columns\n",
    "stemmirna_surv = pd.merge(stemmirna, surv, on='sample', how='outer')\n",
    "# remove the '_PATIENT' column\n",
    "stemmirna_surv = stemmirna_surv.drop(columns='_PATIENT')\n",
    "# Move the OS, OS.time to second and third columns of the dataframe\n",
    "cols = list(stemmirna_surv.columns)\n",
    "cols = [cols[0]] + [cols[-1]] + [cols[-2]] + cols[1:-2]\n",
    "stemmirna_surv = stemmirna_surv[cols]\n",
    "# replace the NaN values with 0\n",
    "stemmirna_surv = stemmirna_surv.fillna(0)\n",
    "stemmirna_surv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNA Methylation + miRNA + Survival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the hvdnameth and stemmirna_surv dataframes using the 'sample' column as the key but keep samples from both dataframes\n",
    "# merge the dataframes\n",
    "# dnameth_stemmirna_surv = pd.merge(stemmirna_surv, hvdnameth, on='sample', how='right')\n",
    "dnameth_stemmirna_surv = pd.merge(stemmirna_surv, hvdnameth, on='sample', how='outer')\n",
    "# replace the NaN values with 0\n",
    "dnameth_stemmirna_surv = dnameth_stemmirna_surv.fillna(0)\n",
    "dnameth_stemmirna_surv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gene Expression + DNA Methylation + miRNA + Survival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the genedataframe and dnameth_stemmirna_surv dataframes using the 'sample' column but keep samples from both dataframes\n",
    "# merge the dataframes\n",
    "gene_dnameth_stemmirna_surv = pd.merge(dnameth_stemmirna_surv, genedataframe, on='sample', how='outer')\n",
    "# replace the NaN values with 0\n",
    "gene_dnameth_stemmirna_surv = gene_dnameth_stemmirna_surv.fillna(0)\n",
    "gene_dnameth_stemmirna_surv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Combined Data to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframe to a csv file without the index column\n",
    "gene_dnameth_stemmirna_surv.to_csv(f'{folders[Indx]}_preprocessed_combined.csv', index=False)\n",
    "print('File saved as: {}_preprocessed_combined.csv'.format(folders[Indx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the combined 3 Modal (miRNA, DNA Methyl, Gene Expr) data and add the clinical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of folder names that will be used to look for the files\n",
    "folders = ['TCGA-ACC', 'TCGA-BLCA', 'TCGA-BRCA', 'TCGA-CESC', 'TCGA-CHOL', 'TCGA-COAD', 'TCGA-DLBC',\n",
    " 'TCGA-ESCA', 'TCGA-GBM', 'TCGA-HNSC', 'TCGA-KICH', 'TCGA-KIRC', 'TCGA-KIRP', 'TCGA-LAML', 'TCGA-LGG',\n",
    "  'TCGA-LIHC', 'TCGA-LUAD', 'TCGA-LUSC', 'TCGA-MESO', 'TCGA-OV', 'TCGA-PAAD', 'TCGA-PCPG', 'TCGA-PRAD',\n",
    "   'TCGA-READ', 'TCGA-SARC', 'TCGA-SKCM', 'TCGA-STAD', 'TCGA-TGCT', 'TCGA-THCA', 'TCGA-THYM', 'TCGA-UCEC',\n",
    "    'TCGA-UCS', 'TCGA-UVM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index values that will be used for each cancer type\n",
    "# change this to process a specific cancer type (0-32)\n",
    "Indx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the csv and verify the dimensions of the saved file\n",
    "preprocessed_data = pd.read_csv('{}_preprocessed_combined.csv'.format(folders[Indx]))\n",
    "print('Opening file: {}_preprocessed_combined.csv'.format(folders[Indx]))\n",
    "print(preprocessed_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data from the csv file and confirm the shape\n",
    "clinical = pd.read_csv(f\"{folders[Indx]}_clinical_data.csv\")\n",
    "print(\"Loading file: {}_clinical_data.csv\".format(folders[Indx]))\n",
    "print(clinical.shape)\n",
    "clinical.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the preprocessed_data and clinical dataframes using the 'sample' column but keep samples from both dataframes\n",
    "combined_data_outer = pd.merge(preprocessed_data, clinical, on='sample', how='outer')\n",
    "# replace the NaN values with 0\n",
    "combined_data_outer = combined_data_outer.fillna(0)\n",
    "print(combined_data_outer.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the preprocessed_data and clinical dataframes using the 'sample' column but keep samples from both dataframes\n",
    "combined_data_inner = pd.merge(preprocessed_data, clinical, on='sample', how='inner')\n",
    "# replace the NaN values with 0\n",
    "combined_data_inner = combined_data_inner.fillna(0)\n",
    "print(combined_data_inner.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the samples different in the two dataframes\n",
    "diff_samples = combined_data_outer[~combined_data_outer['sample'].isin(combined_data_inner['sample'])]\n",
    "diff_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move the columns age\tgender\trace\ttumor_stage to fourth, fifth, sixth and seventh columns in combined_data_inner\n",
    "cols = list(combined_data_inner.columns)\n",
    "cols = cols[:3] + cols[-4:] + cols[3:-4]\n",
    "combined_data_inner = combined_data_inner[cols]\n",
    "combined_data_inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the combined_data_inner dataframe to a csv file without the index column\n",
    "combined_data_inner.to_csv(f'{folders[Indx]}_preprocessed_4modald_mR_Gen_DMeth_Clin.csv', index=False)\n",
    "print('File saved as: {}_preprocessed_4modald_mR_Gen_DMeth_Clin.csv'.format(folders[Indx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide the data into training and testing sets and save to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of folder names that will be used to look for the files\n",
    "folders = ['TCGA-ACC', 'TCGA-BLCA', 'TCGA-BRCA', 'TCGA-CESC', 'TCGA-CHOL', 'TCGA-COAD', 'TCGA-DLBC',\n",
    " 'TCGA-ESCA', 'TCGA-GBM', 'TCGA-HNSC', 'TCGA-KICH', 'TCGA-KIRC', 'TCGA-KIRP', 'TCGA-LAML', 'TCGA-LGG',\n",
    "  'TCGA-LIHC', 'TCGA-LUAD', 'TCGA-LUSC', 'TCGA-MESO', 'TCGA-OV', 'TCGA-PAAD', 'TCGA-PCPG', 'TCGA-PRAD',\n",
    "   'TCGA-READ', 'TCGA-SARC', 'TCGA-SKCM', 'TCGA-STAD', 'TCGA-TGCT', 'TCGA-THCA', 'TCGA-THYM', 'TCGA-UCEC',\n",
    "    'TCGA-UCS', 'TCGA-UVM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index values that will be used for each cancer type\n",
    "# change this to process a specific cancer type (0-32)\n",
    "Indx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data from the csv file and confirm the shape\n",
    "preprocessed_4modald_mR_Gen_DMeth_Clin = pd.read_csv(f'{folders[Indx]}_preprocessed_4modald_mR_Gen_DMeth_Clin.csv')\n",
    "print(\"Loading file: {}_preprocessed_4modald_mR_Gen_DMeth_Clin.csv\".format(folders[Indx]))\n",
    "print(preprocessed_4modald_mR_Gen_DMeth_Clin.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly divide the data into training (80%) and testing (20%0) sets and save to separate CSVs\n",
    "# split the data into training and testing sets\n",
    "train, test = train_test_split(preprocessed_4modald_mR_Gen_DMeth_Clin, test_size=0.2)\n",
    "print(train.shape, test.shape)\n",
    "# save the training and testing sets to csv files\n",
    "train.to_csv(f'{folders[Indx]}_preprocessed_train_4modald_mR_Gen_DMeth_Clin.csv', index=False)\n",
    "test.to_csv(f'{folders[Indx]}_preprocessed_test_4modald_mR_Gen_DMeth_Clin.csv', index=False)\n",
    "print('Files saved as: {}_preprocessed_train_4modald_mR_Gen_DMeth_Clin.csv and {}_preprocessed_test_4modald_mR_Gen_DMeth_Clin.csv'.format(folders[Indx], folders[Indx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ----------- End of Individual Cancer Data Preprocessing -----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine the Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of folder names that will be used to look for the files\n",
    "folders = ['TCGA-ACC', 'TCGA-BLCA', 'TCGA-BRCA', 'TCGA-CESC', 'TCGA-CHOL', 'TCGA-COAD', 'TCGA-DLBC',\n",
    " 'TCGA-ESCA', 'TCGA-GBM', 'TCGA-HNSC', 'TCGA-KICH', 'TCGA-KIRC', 'TCGA-KIRP', 'TCGA-LAML', 'TCGA-LGG',\n",
    "  'TCGA-LIHC', 'TCGA-LUAD', 'TCGA-LUSC', 'TCGA-MESO', 'TCGA-OV', 'TCGA-PAAD', 'TCGA-PCPG', 'TCGA-PRAD',\n",
    "   'TCGA-READ', 'TCGA-SARC', 'TCGA-SKCM', 'TCGA-STAD', 'TCGA-TGCT', 'TCGA-THCA', 'TCGA-THYM', 'TCGA-UCEC',\n",
    "    'TCGA-UCS', 'TCGA-UVM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate all the preprocessed training data files in folders list\n",
    "# initialize empty dataframe\n",
    "train = pd.DataFrame()\n",
    "# concatenate all the dataframes in the list\n",
    "for i in range(0, len(folders)):\n",
    "    print(i, folders[i])\n",
    "    train_file = pd.read_csv(f'{folders[i]}_preprocessed_train_4modald_mR_Gen_DMeth_Clin.csv')\n",
    "    train = pd.concat([train, train_file])\n",
    "    train = train.reset_index(drop=True)\n",
    "    print('train shape:', train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train shape:', train.shape)\n",
    "train.head()\n",
    "# remove the NaN values\n",
    "train = train.fillna(0)\n",
    "print('train shape:', train.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the rows having OS.time=0\n",
    "print(train[train['OS.time'] == 0])\n",
    "# remove the rows having OS.time=0\n",
    "Data_with_OS_time_0 = train[train['OS.time'] == 0]\n",
    "OS_time_0 = train[train['OS.time'] == 0].index\n",
    "train = train.drop(OS_time_0)\n",
    "print('train shape:', train.shape, ', Data_with_OS_time_0 shape:', Data_with_OS_time_0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the rows having all zero values\n",
    "print(train[(train.iloc[:, 3:] == 0).all(axis=1)])\n",
    "# remove the rows having all zero values\n",
    "train = train[~(train.iloc[:, 3:] == 0).all(axis=1)]\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the train data to a csv file\n",
    "train.to_csv('Combined_Train_Data_4modald_mR_Gen_DMeth_Clin.csv', index=False)\n",
    "print('File saved as: Combined_Train_Data_4modald_mR_Gen_DMeth_Clin.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Protein Expression data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the train data\n",
    "train_data = pd.read_csv('Combined_Train_Data_4modald_mR_Gen_DMeth_Clin.csv')\n",
    "print('Opening file: Combined_Train_Data_4modald_mR_Gen_DMeth_Clin.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Sanity Check: see if there are all zeros in a column and remove the constant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.shape)\n",
    "# check if there are any columns that have all zero values except the first three columns\n",
    "print(train_data.columns[(train_data == 0).all()])\n",
    "print(train_data.columns[3:][(train_data.iloc[:, 3:] == 0).all()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view all rows of the printed columns\n",
    "with pd.option_context('display.max_rows', None):\n",
    "    print(train_data[['sample', 'hsa-mir-4297', 'hsa-mir-1302-8', 'hsa-mir-4293', 'hsa-mir-1184-3', 'hsa-mir-646', 'hsa-let-7a-3', 'hsa-mir-4252', 'hsa-mir-548i-2', 'hsa-mir-4330', 'hsa-mir-548a-1', 'hsa-mir-5787']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the constant features\n",
    "from feature_engine.selection import DropConstantFeatures\n",
    "sel1 = DropConstantFeatures(tol=1, variables=None, missing_values='raise')\n",
    "sel1.fit(train_data)\n",
    "data_train = sel1.transform(train_data)\n",
    "data_train.shape\n",
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_train.shape)\n",
    "# check if there are any columns that have all zero values except the first three columns\n",
    "print(data_train.columns[(data_train == 0).all()])\n",
    "print(data_train.columns[3:][(data_train.iloc[:, 3:] == 0).all()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Get the Sample names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the sample names from train data\n",
    "samples = data_train['sample']\n",
    "print(samples.shape)\n",
    "# sort the samples\n",
    "samples = samples.sort_values()\n",
    "pd.set_option('display.max_rows', None)\n",
    "# reset the index\n",
    "samples = samples.reset_index(drop=True)\n",
    "print(samples)\n",
    "print(len(samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. The Protein Expression data for all cancer types is in a folder with patient_IDs as subdirectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the path of data folder\n",
    "path = 'path_to_raw_data'\n",
    "# list the number of folders in the data folder\n",
    "import os\n",
    "folders = os.listdir(path)\n",
    "folders.sort()\n",
    "print(\"List of Protein Expre data subfolders:\", folders)\n",
    "print(\"Total Patients:\", len(folders))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Match the Sample names from other features with the patient_IDs having Protein Expression data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the elements of samples and search through all subfolders of the path for a file starting with the sample name as samples*.tsv\n",
    "# initialize empty dataframe\n",
    "data = pd.DataFrame()\n",
    "# iterate through the samples\n",
    "for i in range(0, len(samples)):\n",
    "    # iterate through the folders\n",
    "    for j in range(0, len(folders)):\n",
    "        # if string of sample except last 4 characters is the same as the folder name\n",
    "        if samples[i][:-4] == folders[j]:\n",
    "            print(j+1, 'Looking {} in folder {}'.format(samples[i], folders[j]))\n",
    "            # get the list of files in the folder that has this structure: '{path}/{folders[j]}/'Protein Expression Quantification'/*'\n",
    "            file = glob.glob(f'{path}/{folders[j]}/Protein Expression Quantification/*/{samples[i]}*')\n",
    "            # read the file.tsv if it exists\n",
    "            if len(file) > 0:\n",
    "                print(file)\n",
    "                # read the file columns having names 'AGID' and 'mutation'\n",
    "                df = pd.read_csv(file[0], sep='\\t', usecols=['AGID', 'protein_expression'])\n",
    "                print('df shape: ', df.shape)\n",
    "                # transpose the dataframe set AGID column as the column names and protein_expression as the values\n",
    "                df = df.transpose()\n",
    "                df = df.reset_index()\n",
    "                # set the columns to the first row\n",
    "                df.columns = df.iloc[0]\n",
    "                # remove the first row\n",
    "                df = df[1:]\n",
    "                # rename the first column to 'sample'\n",
    "                df = df.rename(columns={f'{df.columns[0]}':'sample'})\n",
    "                # set the sample name as the sample[i]\n",
    "                df['sample'] = samples[i]\n",
    "                # concatenate the df to data dataframe\n",
    "                data = pd.concat([data, df])\n",
    "                # reset the index\n",
    "                data = data.reset_index(drop=True)\n",
    "                print('data shape: ', data.shape)\n",
    "            else:\n",
    "                print('File not found in folder:', folders[j])\n",
    "#save the data to a csv file\n",
    "data.to_csv('Protein_Expression_Train_Data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Impute the NANs with the mean of the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the second column onwards, impute the NANs with the mean of the column\n",
    "data.iloc[:, 1:] = data.iloc[:, 1:].apply(lambda x: x.fillna(x.mean()), axis=0)\n",
    "# check if there are any NaN values\n",
    "print(data.isnull().sum().sum())\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the data to a csv file\n",
    "data.to_csv('Protein_Expression_Train_Data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Drop Constant Features (i.e. >99.8% similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_expr = data\n",
    "print(protein_expr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine.selection import DropConstantFeatures\n",
    "sel1 = DropConstantFeatures(tol=0.998, variables=None, missing_values='raise')\n",
    "sel1.fit(protein_expr)\n",
    "protein_expr = sel1.transform(protein_expr)\n",
    "protein_expr.shape\n",
    "protein_expr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Remove Colinear Features (i.e. >80% correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temporary remove the first column for later adding it to the first column\n",
    "protein_expr1 = protein_expr.iloc[:, 1:]\n",
    "# check the variable format with pandas dtypes.\n",
    "print(protein_expr1.dtypes)\n",
    "# convert the variable to numerical variables\n",
    "protein_expr1 = protein_expr1.astype(float)\n",
    "# check the variable format with pandas dtypes.\n",
    "print(protein_expr1.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove correlated features\n",
    "from feature_engine.selection import SmartCorrelatedSelection\n",
    "sel4 = SmartCorrelatedSelection(\n",
    "    variables=None,\n",
    "    method=\"pearson\",\n",
    "    threshold=0.8,\n",
    "    missing_values=\"raise\",\n",
    "    selection_method=\"variance\",\n",
    "    estimator=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(protein_expr1.shape)\n",
    "sel4.fit(protein_expr1)\n",
    "protexpr = sel4.transform(protein_expr1)\n",
    "protexpr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Add OS labels and save the Protein Expression data as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the 'sample' column from protein_expr to the first column of protexpr\n",
    "protexpr.insert(0, 'sample', protein_expr['sample'])\n",
    "# add OS and OS.time columns from data_train to the protexpr dataframe based on the sample name\n",
    "protexpr = pd.merge(protexpr, data_train[['sample', 'OS', 'OS.time']], on='sample', how='inner')\n",
    "# Move the OS, OS.time to second and third columns of the dataframe\n",
    "cols = list(protexpr.columns)\n",
    "cols = [cols[0]] + [cols[-1]] + [cols[-2]] + cols[1:-2]\n",
    "protexpr = protexpr[cols]\n",
    "print(protexpr.shape)\n",
    "protexpr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the data to a csv file\n",
    "protexpr.to_csv('Protein_Expression_Train_Data_processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the distribution of the OS.time variable\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data_train['OS.time'], kde=True)\n",
    "plt.title('Distribution of OS.time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the distribution of the first 4 variables\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data_train[data_train.columns[3]], kde=True)\n",
    "sns.histplot(data_train[data_train.columns[4]], kde=True)\n",
    "sns.histplot(data_train[data_train.columns[5]], kde=True)\n",
    "sns.histplot(data_train[data_train.columns[6]], kde=True)\n",
    "plt.title('Distribution of ' + data_train.columns[3] + ', ' + data_train.columns[4] + ', ' + data_train.columns[5] + ', ' + data_train.columns[6])\n",
    "plt.legend([data_train.columns[3], data_train.columns[4], data_train.columns[5], data_train.columns[6]])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data_train.head() for 3rd columns onwards\n",
    "data_train.iloc[:, 7:].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data with zero mean and unit variance across the samples\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# initialize the standard scaler\n",
    "scaler = StandardScaler()\n",
    "# fit and transform the data without the first three columns\n",
    "data_train.iloc[:, 7:] = scaler.fit_transform(data_train.iloc[:, 7:])\n",
    "print('train shape:', data_train.shape)\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the distribution of the first 4 variables\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data_train[data_train.columns[7]], kde=True)\n",
    "sns.histplot(data_train[data_train.columns[8]], kde=True)\n",
    "sns.histplot(data_train[data_train.columns[9]], kde=True)\n",
    "sns.histplot(data_train[data_train.columns[10]], kde=True)\n",
    "plt.title('Distribution of ' + data_train.columns[7] + ', ' + data_train.columns[8] + ', ' + data_train.columns[9] + ', ' + data_train.columns[10])\n",
    "plt.legend([data_train.columns[7], data_train.columns[8], data_train.columns[9], data_train.columns[10]])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Add the Protein Expression data to the combined data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the protexpr and data_train dataframes using the 'sample', OS.time, and OS columns as the key but keep samples from both dataframes\n",
    "print(\"Protein Expression Data shape: \", protexpr.shape)\n",
    "print(\"Train Data shape: \", data_train.shape)\n",
    "# merge the dataframes\n",
    "protexpr_plus_train_data = pd.merge(protexpr, data_train, on=['sample', 'OS.time', 'OS'], how='outer')\n",
    "print(\"Combined Train Data shape: \", protexpr_plus_train_data.shape)\n",
    "# replace the NaN values with 0\n",
    "protexpr_plus_train_data = protexpr_plus_train_data.fillna(0)\n",
    "print(protexpr_plus_train_data.shape)\n",
    "protexpr_plus_train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protexpr_plus_train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. Save the combined data to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv\n",
    "protexpr_plus_train_data.to_csv('5Modal_Train_Data_mR_Gen_DMeth_Clin_Prot.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protexpr_plus_train_data = pd.read_csv('5Modal_Train_Data_mR_Gen_DMeth_Clin_Prot.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(protexpr_plus_train_data.shape)\n",
    "protexpr_plus_train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the columns having names 'age', 'gender', 'race', 'tumor_stage'\n",
    "# Get the current column names as a list\n",
    "cols = list(protexpr_plus_train_data.columns)\n",
    "# Define the columns to move and their new positions\n",
    "cols_to_move = ['age', 'gender', 'race', 'tumor_stage']\n",
    "new_positions = [3, 4, 5, 6]\n",
    "# Remove the columns to move from the current column list\n",
    "for col in cols_to_move:\n",
    "    cols.remove(col)\n",
    "# Insert the columns to move at their new positions\n",
    "for col, pos in zip(cols_to_move, new_positions):\n",
    "    cols.insert(pos, col)\n",
    "# Reindex the DataFrame with the new column order\n",
    "protexpr_plus_train_data_tmp = protexpr_plus_train_data[cols]\n",
    "protexpr_plus_train_data_tmp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(protexpr_plus_train_data_tmp.shape)\n",
    "fiveMod_Train_mR_Gen_DMeth_Clin_Prot = protexpr_plus_train_data_tmp\n",
    "print(fiveMod_Train_mR_Gen_DMeth_Clin_Prot.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. Sanity Check: verify that there are no zero-valued rows or columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if there are any rows that have all Zero values\n",
    "# find the rows having all zero values\n",
    "print(fiveMod_Train_mR_Gen_DMeth_Clin_Prot[(fiveMod_Train_mR_Gen_DMeth_Clin_Prot.iloc[:, 7:] == 0).all(axis=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if there are any columns that have all Zero values\n",
    "# find the columns having all zero values\n",
    "print(fiveMod_Train_mR_Gen_DMeth_Clin_Prot.columns[(fiveMod_Train_mR_Gen_DMeth_Clin_Prot == 0).all()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the fiveMod_Train_mR_Gen_DMeth_Clin_Prot to csv\n",
    "fiveMod_Train_mR_Gen_DMeth_Clin_Prot.to_csv('5Modal_Train_Data_mR_Gen_DMeth_Clin_Prot.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15. k-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the patient names from sample column\n",
    "patientnames=fiveMod_Train_mR_Gen_DMeth_Clin_Prot['sample']\n",
    "print(len(patientnames))\n",
    "print(patientnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate k-fold cross validation splits for the data, k=10\n",
    "from sklearn.model_selection import KFold\n",
    "# initialize the kfold object\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "folds = list(kf.split(patientnames))\n",
    "folds_array = np.zeros((len(patientnames), 10))\n",
    "for i in range(10):\n",
    "    folds_array[folds[i][1], i] = 1\n",
    "folds_df = pd.DataFrame(folds_array, columns=['fold_{}'.format(i) for i in range(1,11)])\n",
    "folds_df.index = patientnames\n",
    "#replace 0 with Train and 1 with Test\n",
    "folds_df = folds_df.replace(0, 'Train')\n",
    "folds_df = folds_df.replace(1, 'Test')\n",
    "folds_df.to_csv('pnas_fiveMod_Train_mR_Gen_DMeth_Clin_Prot_splits.csv')\n",
    "folds_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15. Save the Train Data in a .pkl file for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the samples, vital_status (OS) and labels (OS.time) to numpy.ndarray\n",
    "samples = fiveMod_Train_mR_Gen_DMeth_Clin_Prot['sample'].values\n",
    "vital_status = fiveMod_Train_mR_Gen_DMeth_Clin_Prot['OS'].values\n",
    "survival = fiveMod_Train_mR_Gen_DMeth_Clin_Prot['OS.time'].values\n",
    "print('samples:', samples, 'vital_status:', vital_status, 'labels:', survival)\n",
    "print('samples shape:', samples.shape, 'vital_status shape:', vital_status.shape, 'survival shape:', survival.shape)\n",
    "# convert survival to df\n",
    "survival_df = pd.DataFrame(survival, columns=['Labels'])\n",
    "survival_df.index = samples\n",
    "survival_df = survival_df.astype(float)\\\n",
    "# convert vital_status to df\n",
    "vital_status_df = pd.DataFrame(vital_status, columns=['Vital_status'])\n",
    "vital_status_df.index = samples\n",
    "vital_status_df = vital_status_df.astype(float)\n",
    "# remove the vital_status (OS) and labels (OS.time) from the dataframe\n",
    "print('train shape:', fiveMod_Train_mR_Gen_DMeth_Clin_Prot.shape)\n",
    "omic = fiveMod_Train_mR_Gen_DMeth_Clin_Prot.drop(columns=['sample', 'OS', 'OS.time'])\n",
    "omic.index = samples\n",
    "print('omic shape:', omic.shape)\n",
    "omic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cv = {}\n",
    "data_cv['cv_splits'] = {}\n",
    "\n",
    "for i in range(10):\n",
    "    # add and store data for each fold in the data_cv dictionary\n",
    "    data_cv['cv_splits'][i+1] = {}\n",
    "    data_cv['cv_splits'][i+1]['train'] = {}\n",
    "    data_cv['cv_splits'][i+1]['test'] = {}\n",
    "    data_cv['cv_splits'][i+1]['train'] = {\n",
    "        'x_patname': folds_df[folds_df['fold_{}'.format(i+1)] == 'Train'].index.values.tolist(),\n",
    "        'x_omic': omic.loc[folds_df[folds_df['fold_{}'.format(i+1)] == 'Train'].index.values.tolist()].values,\n",
    "        'e': np.squeeze(vital_status_df.loc[folds_df[folds_df['fold_{}'.format(i+1)] == 'Train'].index.values.tolist()].values),\n",
    "        't': np.squeeze(survival_df.loc[folds_df[folds_df['fold_{}'.format(i+1)] == 'Train'].index.values.tolist()].values)\n",
    "    }\n",
    "    data_cv['cv_splits'][i+1]['test'] = {\n",
    "        'x_patname': folds_df[folds_df['fold_{}'.format(i+1)] == 'Test'].index.values.tolist(),\n",
    "        'x_omic': omic.loc[folds_df[folds_df['fold_{}'.format(i+1)] == 'Test'].index.values.tolist()].values,\n",
    "        'e': np.squeeze(vital_status_df.loc[folds_df[folds_df['fold_{}'.format(i+1)] == 'Test'].index.values.tolist()].values),\n",
    "        't': np.squeeze(survival_df.loc[folds_df[folds_df['fold_{}'.format(i+1)] == 'Test'].index.values.tolist()].values)\n",
    "    }\n",
    "\n",
    "# save the dictionary to a .pkl file\n",
    "with open('omic_10cv.pkl', 'wb') as f:\n",
    "    pickle.dump(data_cv, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERIFY THE PKL FILE\n",
    "import pickle\n",
    "data_cv = pickle.load(open('omic_10cv.pkl', 'rb'))\n",
    "data_cv_splits = data_cv['cv_splits']\n",
    "for k, data in data_cv_splits.items():\n",
    "\tprint(\"*******************************************\")\n",
    "\tprint(\"************** SPLIT (%d/%d) **************\" % (k, len(data_cv_splits.items())))\n",
    "\tprint(\"*******************************************\")\n",
    "\tif k == 1:\n",
    "\t\tprint(data_cv_splits)\n",
    "\tprint(len(data_cv_splits[k]['train']['x_patname']), (data_cv_splits[k]['train']['e']).shape, (data_cv_splits[k]['train']['t']).shape, (data_cv_splits[k]['train']['x_omic']).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('data_cv.keys: ', data_cv.keys())\n",
    "print(\"data_cv['cv_splits'].keys: \", data_cv['cv_splits'].keys())\n",
    "print(\"data_cv['cv_splits'][1]: \", data_cv['cv_splits'][1].keys())\n",
    "print(\"data_cv['cv_splits'][1]['train']: \", data_cv['cv_splits'][1]['train'].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since the single file is ~45GB, we will divide the data for each fold and save to separate .pkl files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 11):\n",
    "    # create a new dictionary for each fold\n",
    "    data_dict = {\n",
    "        'cv_splits': {\n",
    "            i: {\n",
    "                'train': data['cv_splits'][i]['train'],\n",
    "                'test': data['cv_splits'][i]['test']\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    with open('omic_10cv_fold_{}.pkl'.format(i), 'wb') as f:\n",
    "        pickle.dump(data_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load .pkl file for each fold and verify the data lengths\n",
    "for k in range(1, 11):\n",
    "    data = pickle.load(open('omic_10cv_fold_{}.pkl'.format(k), 'rb'))\n",
    "    print(\"*******************************************\")\n",
    "    print(\"************** SPLIT (%d/%d) **************\" % (k, 10))\n",
    "    print(\"*******************************************\")\n",
    "    print('data.keys: ', data.keys())\n",
    "    print(\"data['cv_splits'].keys: \", data['cv_splits'].keys())\n",
    "    print(\"data['cv_splits'][1]: \", data['cv_splits'][k].keys())\n",
    "    print(\"data['cv_splits'][1]['train']: \", data['cv_splits'][k]['train'].keys())\n",
    "    print(\"data['cv_splits'][1]['test']: \", data['cv_splits'][k]['test'].keys())\n",
    "    print(len(data['cv_splits'][k]['train']['x_patname']),\n",
    "            (data['cv_splits'][k]['train']['e']).shape,\n",
    "            (data['cv_splits'][k]['train']['t']).shape,\n",
    "            (data['cv_splits'][k]['train']['x_omic']).shape\n",
    "    )\n",
    "    print(len(data['cv_splits'][k]['test']['x_patname']), (data['cv_splits'][k]['test']['e']).shape, (data['cv_splits'][k]['test']['t']).shape, (data['cv_splits'][k]['test']['x_omic']).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  DNA Mutation data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiveMod_Train_mR_Gen_DMeth_Clin_Prot.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Get the Sample names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the sample names from train data\n",
    "samples = fiveMod_Train_mR_Gen_DMeth_Clin_Prot['sample']\n",
    "print(samples.shape)\n",
    "# sort the samples\n",
    "samples = samples.sort_values()\n",
    "pd.set_option('display.max_rows', None)\n",
    "# reset the index\n",
    "samples = samples.reset_index(drop=True)\n",
    "print(samples)\n",
    "print(len(samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Load the DNA Mutation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load DNA Mut data from DNA_union_df.csv\n",
    "DNA_mut = pd.read_csv('DNA_union_df.csv')\n",
    "print(DNA_mut.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DNA_mut.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Match the Sample names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(samples.shape)\n",
    "print(DNA_mut['SampleID'].shape)\n",
    "# get the DNA_mut data based on DNA_mut['SampleID']=samples[:-4]\n",
    "DNA_mut_data = DNA_mut[DNA_mut['SampleID'].isin(samples.str[:-4])]\n",
    "print(DNA_mut_data.shape)\n",
    "DNA_mut_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Impute the NANs with the mean of the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if there are any NANs for second column onwards\n",
    "print(DNA_mut_data.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Drop Constant Features (i.e. >100% similarity (all zeros))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if any column has all zero values\n",
    "print(DNA_mut_data.columns[(DNA_mut_data == 0).all()])\n",
    "# get all the column names having all zero values in alist\n",
    "all_zero_columns = DNA_mut_data.columns[(DNA_mut_data == 0).all()].tolist()\n",
    "print(len(all_zero_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine.selection import DropConstantFeatures\n",
    "print(DNA_mut_data.shape)\n",
    "sel1 = DropConstantFeatures(tol=1, variables=None, missing_values='raise')\n",
    "sel1.fit(DNA_mut_data)\n",
    "DNA_muts = sel1.transform(DNA_mut_data)\n",
    "print(DNA_muts.shape)\n",
    "DNA_muts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Remove Colinear Features (i.e. >80% correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temporary remove the first column for later adding it to the first column\n",
    "DNA_muts1 = DNA_muts.iloc[:, 1:]\n",
    "# check the variable format with pandas dtypes.\n",
    "print(DNA_muts1.dtypes)\n",
    "# convert the variable to numerical variables\n",
    "DNA_muts1 = DNA_muts1.astype(float)\n",
    "# check the variable format with pandas dtypes.\n",
    "print(DNA_muts1.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove correlated features\n",
    "from feature_engine.selection import SmartCorrelatedSelection\n",
    "sel5 = SmartCorrelatedSelection(\n",
    "    variables=None,\n",
    "    method=\"pearson\",\n",
    "    threshold=0.8,\n",
    "    missing_values=\"raise\",\n",
    "    selection_method=\"variance\",\n",
    "    estimator=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(DNA_muts1.shape)\n",
    "sel5.fit(DNA_muts1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DNAmuts = sel5.transform(DNA_muts1)\n",
    "DNAmuts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DNAmuts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the DNAmuts data to a csv file\n",
    "DNAmuts.to_csv('DNA_Mut_Train_Data_processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load file DNA_Mut_Train_Data_processed.csv\n",
    "DNAmuts = pd.read_csv('DNA_Mut_Train_Data_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(DNAmuts.shape)\n",
    "DNAmuts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_data = DNAmuts\n",
    "print(actual_data.shape)\n",
    "actual_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = DNA_muts\n",
    "temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the index of temp\n",
    "temp = temp.reset_index(drop=True)\n",
    "print(temp.shape)\n",
    "temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add 'SampleID' from temp to the first column of actual_data\n",
    "actual_data.insert(0, 'SampleID', temp['SampleID'])\n",
    "print(actual_data.shape)\n",
    "actual_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Add OS labels and save the DNA Mut data as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add 'sample, 'OS', 'OS.time' from four_modal to DNAmuts\n",
    "# get the fiveMod_Train_mR_Gen_DMeth_Clin_Prot['sample', 'OS', 'OS.time'] in a separate df\n",
    "fivemodal_3 = fiveMod_Train_mR_Gen_DMeth_Clin_Prot[['sample', 'OS', 'OS.time']]\n",
    "print(fivemodal_3.shape)\n",
    "print(fivemodal_3.head())\n",
    "# add fivemodal_3['sample'].str[:-4] to fivemodal_3 as first column\n",
    "fivemodal_3.insert(0, 'SampleID', fivemodal_3['sample'].str[:-4])\n",
    "print(fivemodal_3.shape)\n",
    "print(fivemodal_3.head())\n",
    "\n",
    "# add fiveMod_Train_mR_Gen_DMeth_Clin_Prot['sample] to actual_data as first column based on SampleID=sample[:-4]\n",
    "DNAMut_data = pd.merge(actual_data, fivemodal_3[['SampleID', 'sample', 'OS', 'OS.time']], left_on='SampleID', right_on='SampleID', how='inner')\n",
    "print(DNAMut_data.shape)\n",
    "# Move the sample, OS, OS.time to first, second and third columns of the dataframe\n",
    "cols = list(DNAMut_data.columns)\n",
    "cols = [cols[-3]] + [cols[-1]] + [cols[-2]] + cols[0:-3]\n",
    "DNAMut_data = DNAMut_data[cols]\n",
    "print(DNAMut_data.shape)\n",
    "DNAMut_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the duplicate rows\n",
    "print(DNAMut_data['SampleID'].duplicated().sum())\n",
    "print(DNAMut_data['sample'].duplicated().sum())\n",
    "# get the duplicate rows based on DNAMut_data['SampleID']\n",
    "print(DNAMut_data[DNAMut_data['SampleID'].duplicated()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the duplicate rows based on SampleID\n",
    "print(DNAMut_data.shape)\n",
    "DM_data = DNAMut_data.drop_duplicates(subset='sample', keep='first')\n",
    "print(DM_data.shape)\n",
    "print(DM_data['SampleID'].duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the duplicate rows based on SampleID\n",
    "print(DNAMut_data.shape)\n",
    "DM_data = DNAMut_data.drop_duplicates(subset='SampleID', keep='first')\n",
    "print(DM_data.shape)\n",
    "print(DM_data['SampleID'].duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DM_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the SampleID column\n",
    "DM_data = DM_data.drop(columns='SampleID')\n",
    "print(DM_data.shape)\n",
    "DM_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to a csv\n",
    "DM_data.to_csv('DNA_Mut_Train_Data_processed_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the csv and verify the dimensions of the saved file\n",
    "# DM_data = pd.read_csv('DNA_Mut_Train_Data_processed_final.csv')\n",
    "DM_data = pd.read_csv('DNA_Mut_Train_Data_processed_final.csv')\n",
    "print('Opening file: DNA_Mut_Train_Data_processed_final.csv')\n",
    "print(DM_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DM_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Normalize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We don't need to normalize, because the DNA Mut data is already binary, and the 4Modality data is already normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Add the DNA Mut data to the combined data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the DNA Mut and fiveMod_Train_mR_Gen_DMeth_Clin_Prot dataframes using the 'sample', OS.time, and OS columns as the key but keep samples from both dataframes\n",
    "print(\"DNA Mut Data shape: \", DM_data.shape)\n",
    "print(\"5Modal Train Data shape: \", fiveMod_Train_mR_Gen_DMeth_Clin_Prot.shape)\n",
    "# merge the dataframes\n",
    "sixmodal_train_data = pd.merge(DM_data, fiveMod_Train_mR_Gen_DMeth_Clin_Prot, on=['sample', 'OS.time', 'OS'], how='outer')\n",
    "print(\"Combined Train Data shape: \", sixmodal_train_data.shape)\n",
    "# replace the NaN values with 0\n",
    "sixmodal_train_data = sixmodal_train_data.fillna(0)\n",
    "print(sixmodal_train_data.shape)\n",
    "sixmodal_train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the columns having names 'age', 'gender', 'race', 'tumor_stage'\n",
    "# Get the current column names as a list\n",
    "cols = list(sixmodal_train_data.columns)\n",
    "# Define the columns to move and their new positions\n",
    "cols_to_move = ['age', 'gender', 'race', 'tumor_stage']\n",
    "new_positions = [3, 4, 5, 6]\n",
    "# Remove the columns to move from the current column list\n",
    "for col in cols_to_move:\n",
    "    cols.remove(col)\n",
    "# Insert the columns to move at their new positions\n",
    "for col, pos in zip(cols_to_move, new_positions):\n",
    "    cols.insert(pos, col)\n",
    "# Reindex the DataFrame with the new column order\n",
    "sixmodal_train_data_tmp = sixmodal_train_data[cols]\n",
    "sixmodal_train_data_tmp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Save the combined data to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to a csv\n",
    "sixmodal_train_data_tmp.to_csv('6Modal_Train_Data_mR_Gen_DMeth_Clin_Prot_Mut.csv', index=False)\n",
    "print(\"saving to 6Modal_Train_Data_mR_Gen_DMeth_Clin_Prot_Mut.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the csv file\n",
    "sixmodal_train_data = pd.read_csv('6Modal_Train_Data_mR_Gen_DMeth_Clin_Prot_Mut.csv')\n",
    "print('Opening file: 6Modal_Train_Data_mR_Gen_DMeth_Clin_Prot_Mut.csv')\n",
    "print(sixmodal_train_data.shape)\n",
    "sixmodal_train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sixmodal_train_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. Sanity Check: verify that there are no zero-valued rows or columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if there are any rows that have all Zero values\n",
    "# find the rows having all zero values\n",
    "print(sixmodal_train_data[(sixmodal_train_data.iloc[:, 3:] == 0).all(axis=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if there are any columns that have all Zero values\n",
    "# find the columns having all zero values\n",
    "print(sixmodal_train_data.columns[(sixmodal_train_data == 0).all()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. k-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the patient names from sample column\n",
    "patientnames=sixmodal_train_data['sample']\n",
    "print(len(patientnames))\n",
    "print(patientnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate k-fold cross validation splits for the data, k=10\n",
    "from sklearn.model_selection import KFold\n",
    "# initialize the kfold object\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "folds = list(kf.split(patientnames))\n",
    "folds_array = np.zeros((len(patientnames), 10))\n",
    "for i in range(10):\n",
    "    folds_array[folds[i][1], i] = 1\n",
    "folds_df = pd.DataFrame(folds_array, columns=['fold_{}'.format(i) for i in range(1,11)])\n",
    "folds_df.index = patientnames\n",
    "#replace 0 with Train and 1 with Test\n",
    "folds_df = folds_df.replace(0, 'Train')\n",
    "folds_df = folds_df.replace(1, 'Test')\n",
    "folds_df.to_csv('pnas_splits.csv')\n",
    "folds_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15. Save the Train Data in a .pkl file for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the samples, vital_status (OS) and labels (OS.time) to numpy.ndarray\n",
    "samples = sixmodal_train_data['sample'].values\n",
    "vital_status = sixmodal_train_data['OS'].values\n",
    "survival = sixmodal_train_data['OS.time'].values\n",
    "print('samples:', samples, 'vital_status:', vital_status, 'labels:', survival)\n",
    "print('samples shape:', samples.shape, 'vital_status shape:', vital_status.shape, 'survival shape:', survival.shape)\n",
    "# convert survival to df\n",
    "survival_df = pd.DataFrame(survival, columns=['Labels'])\n",
    "survival_df.index = samples\n",
    "survival_df = survival_df.astype(float)\n",
    "# convert vital_status to df\n",
    "vital_status_df = pd.DataFrame(vital_status, columns=['Vital_status'])\n",
    "vital_status_df.index = samples\n",
    "vital_status_df = vital_status_df.astype(float)\n",
    "# remove the vital_status (OS) and labels (OS.time) from the dataframe\n",
    "print('train shape:', sixmodal_train_data.shape)\n",
    "omic = sixmodal_train_data.drop(columns=['sample', 'OS', 'OS.time'])\n",
    "omic.index = samples\n",
    "print('omic shape:', omic.shape)\n",
    "omic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # convert dataframes to numpy arrays\n",
    "data_cv = {}\n",
    "data_cv['cv_splits'] = {}\n",
    "\n",
    "for i in range(10):\n",
    "    # add and store data for each fold in the data_cv dictionary\n",
    "    data_cv['cv_splits'][i+1] = {}\n",
    "    data_cv['cv_splits'][i+1]['train'] = {}\n",
    "    data_cv['cv_splits'][i+1]['test'] = {}\n",
    "    data_cv['cv_splits'][i+1]['train'] = {\n",
    "        'x_patname': folds_df[folds_df['fold_{}'.format(i+1)] == 'Train'].index.values.tolist(),\n",
    "        'x_omic': omic.loc[folds_df[folds_df['fold_{}'.format(i+1)] == 'Train'].index.values.tolist()].values,\n",
    "        'e': np.squeeze(vital_status_df.loc[folds_df[folds_df['fold_{}'.format(i+1)] == 'Train'].index.values.tolist()].values),\n",
    "        't': np.squeeze(survival_df.loc[folds_df[folds_df['fold_{}'.format(i+1)] == 'Train'].index.values.tolist()].values)\n",
    "    }\n",
    "    data_cv['cv_splits'][i+1]['test'] = {\n",
    "        'x_patname': folds_df[folds_df['fold_{}'.format(i+1)] == 'Test'].index.values.tolist(),\n",
    "        'x_omic': omic.loc[folds_df[folds_df['fold_{}'.format(i+1)] == 'Test'].index.values.tolist()].values,\n",
    "        'e': np.squeeze(vital_status_df.loc[folds_df[folds_df['fold_{}'.format(i+1)] == 'Test'].index.values.tolist()].values),\n",
    "        't': np.squeeze(survival_df.loc[folds_df[folds_df['fold_{}'.format(i+1)] == 'Test'].index.values.tolist()].values)\n",
    "    }\n",
    "\n",
    "# save the dictionary to a .pkl file\n",
    "with open('omic_10cv.pkl', 'wb') as f:\n",
    "    pickle.dump(data_cv, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERIFY THE PKL FILE\n",
    "import pickle\n",
    "data_cv = pickle.load(open('omic_10cv.pkl', 'rb'))\n",
    "data_cv_splits = data_cv['cv_splits']\n",
    "\n",
    "for k, data in data_cv_splits.items():\n",
    "\tprint(\"*******************************************\")\n",
    "\tprint(\"************** SPLIT (%d/%d) **************\" % (k, len(data_cv_splits.items())))\n",
    "\tprint(\"*******************************************\")\n",
    "\tif k == 1:\n",
    "\t\tprint(data_cv_splits)\n",
    "\tprint(len(data_cv_splits[k]['train']['x_patname']), (data_cv_splits[k]['train']['e']).shape, (data_cv_splits[k]['train']['t']).shape, (data_cv_splits[k]['train']['x_omic']).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the dictionary names, and sub-dictionary names\n",
    "print('data_cv.keys: ', data_cv.keys())\n",
    "print(\"data_cv['cv_splits'].keys: \", data_cv['cv_splits'].keys())\n",
    "print(\"data_cv['cv_splits'][1]: \", data_cv['cv_splits'][1].keys())\n",
    "print(\"data_cv['cv_splits'][1]['train']: \", data_cv['cv_splits'][1]['train'].keys())\n",
    "print(\"data_cv['cv_splits'][1]['test']: \", data_cv['cv_splits'][1]['test'].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since the single file is ~67GB, we will divide the data for each fold and save to separate .pkl files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 11):\n",
    "    # create a new dictionary for each fold\n",
    "    data_dict = {\n",
    "        'cv_splits': {\n",
    "            i: {\n",
    "                'train': data['cv_splits'][i]['train'],\n",
    "                'test': data['cv_splits'][i]['test']\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # save the dictionary to a .pkl file\n",
    "    with open('train/omic_10cv_fold_{}.pkl'.format(i), 'wb') as f:\n",
    "        pickle.dump(data_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load .pkl file for each fold and verify the data lengths\n",
    "for k in range(1, 11):\n",
    "    data = pickle.load(open('train/omic_10cv_fold_{}.pkl'.format(k), 'rb'))\n",
    "    print(\"*******************************************\")\n",
    "    print(\"************** SPLIT (%d/%d) **************\" % (k, 10))\n",
    "    print(\"*******************************************\")\n",
    "    # print the dictionary names, and sub-dictionary names\n",
    "    print('data.keys: ', data.keys())\n",
    "    print(\"data['cv_splits'].keys: \", data['cv_splits'].keys())\n",
    "    print(\"data['cv_splits'][1]: \", data['cv_splits'][k].keys())\n",
    "    print(\"data['cv_splits'][1]['train']: \", data['cv_splits'][k]['train'].keys())\n",
    "    print(\"data['cv_splits'][1]['test']: \", data['cv_splits'][k]['test'].keys())\n",
    "    print(len(data['cv_splits'][k]['train']['x_patname']),\n",
    "            (data['cv_splits'][k]['train']['e']).shape,\n",
    "            (data['cv_splits'][k]['train']['t']).shape,\n",
    "            (data['cv_splits'][k]['train']['x_omic']).shape\n",
    "    )\n",
    "    print(len(data['cv_splits'][k]['test']['x_patname']), (data['cv_splits'][k]['test']['e']).shape, (data['cv_splits'][k]['test']['t']).shape, (data['cv_splits'][k]['test']['x_omic']).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ----------- End of Train/Validation Data Preprocessing -----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine the Inference data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np\n",
    "import pprint\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from feature_engine.selection import DropDuplicateFeatures, DropConstantFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of folder names that will be used to look for the files\n",
    "folders = ['TCGA-ACC', 'TCGA-BLCA', 'TCGA-BRCA', 'TCGA-CESC', 'TCGA-CHOL', 'TCGA-COAD', 'TCGA-DLBC',\n",
    " 'TCGA-ESCA', 'TCGA-GBM', 'TCGA-HNSC', 'TCGA-KICH', 'TCGA-KIRC', 'TCGA-KIRP', 'TCGA-LAML', 'TCGA-LGG',\n",
    "  'TCGA-LIHC', 'TCGA-LUAD', 'TCGA-LUSC', 'TCGA-MESO', 'TCGA-OV', 'TCGA-PAAD', 'TCGA-PCPG', 'TCGA-PRAD',\n",
    "   'TCGA-READ', 'TCGA-SARC', 'TCGA-SKCM', 'TCGA-STAD', 'TCGA-TGCT', 'TCGA-THCA', 'TCGA-THYM', 'TCGA-UCEC',\n",
    "    'TCGA-UCS', 'TCGA-UVM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate all the preprocessed test data files in folders list\n",
    "# initialize empty dataframe\n",
    "test = pd.DataFrame()\n",
    "# concatenate all the dataframes in the list\n",
    "for i in range(0, len(folders)):\n",
    "    print(i, folders[i])\n",
    "    test_file = pd.read_csv(f'{folders[i]}_preprocessed_test_4modald_mR_Gen_DMeth_Clin.csv')\n",
    "    test = pd.concat([test, test_file])\n",
    "    test = test.reset_index(drop=True)\n",
    "    print('test shape:', test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('test shape:', test.shape)\n",
    "test.head()\n",
    "# remove the NaN values\n",
    "test = test.fillna(0)\n",
    "print('test shape:', test.shape)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the rows having OS.time=0\n",
    "print(test[test['OS.time'] == 0])\n",
    "# remove the rows having OS.time=0\n",
    "Test_Data_with_OS_time_0 = test[test['OS.time'] == 0]\n",
    "OS_time_0 = test[test['OS.time'] == 0].index\n",
    "test = test.drop(OS_time_0)\n",
    "print('test shape:', test.shape, ', Test_Data_with_OS_time_0 shape:', Test_Data_with_OS_time_0.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the rows having all zero values\n",
    "print(test[(test.iloc[:, 3:] == 0).all(axis=1)])\n",
    "# remove the rows having all zero values\n",
    "test = test[~(test.iloc[:, 3:] == 0).all(axis=1)]\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the test data to a csv file\n",
    "test.to_csv('Combined_Test_Data_4modald_mR_Gen_DMeth_Clin.csv', index=False)\n",
    "print('File saved as: Combined_Test_Data_4modald_mR_Gen_DMeth_Clin.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Protein Expression data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test\n",
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the test data\n",
    "test_data = pd.read_csv('Combined_Test_Data_4modald_mR_Gen_DMeth_Clin.csv')\n",
    "print('Opening file: Combined_Test_Data_4modald_mR_Gen_DMeth_Clin.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Sanity Check: see if there are all zeros in a column and remove the constant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_data.shape)\n",
    "test_data.head()\n",
    "# check if there are any columns that have all zero values except the first three columns\n",
    "print(test_data.columns[(test_data == 0).all()])\n",
    "print(test_data.columns[3:][(test_data.iloc[:, 3:] == 0).all()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are features that have all zero values, but to keep the data consistent, we will only remove those features that have been removed from the training data\n",
    "# Features removed from training data: 'hsa-mir-4297', 'hsa-mir-1302-8', 'hsa-mir-548f-5', 'hsa-mir-1184-3', 'hsa-let-7a-3', 'hsa-mir-4280', 'hsa-mir-548i-2', 'hsa-mir-4499', 'hsa-mir-4330', 'hsa-mir-3975', 'hsa-mir-5787'\n",
    "# view all rows of the printed columns\n",
    "with pd.option_context('display.max_rows', None):\n",
    "    print(test_data[['sample', 'hsa-mir-4297', 'hsa-mir-1302-8', 'hsa-mir-4293', 'hsa-mir-1184-3', 'hsa-mir-646', 'hsa-let-7a-3', 'hsa-mir-4252', 'hsa-mir-548i-2', 'hsa-mir-4330', 'hsa-mir-548a-1', 'hsa-mir-5787']])\n",
    "    # print the rows in the above columns that have non-zero values\n",
    "    colms = ['sample', 'hsa-mir-4297', 'hsa-mir-1302-8', 'hsa-mir-4293', 'hsa-mir-1184-3', 'hsa-mir-646', 'hsa-let-7a-3', 'hsa-mir-4252', 'hsa-mir-548i-2', 'hsa-mir-4330', 'hsa-mir-548a-1', 'hsa-mir-5787']\n",
    "    for column in colms[1:]:  # Skip the 'sample' column\n",
    "        non_zero_samples = test_data.loc[test_data[column] != 0, 'sample']\n",
    "        print(f'Samples in {column} with non-zero values:')\n",
    "        print(non_zero_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_data.shape)\n",
    "test_data = test_data.drop(columns=['hsa-mir-4297', 'hsa-mir-1302-8', 'hsa-mir-4293', 'hsa-mir-1184-3', 'hsa-mir-646', 'hsa-let-7a-3', 'hsa-mir-4252', 'hsa-mir-548i-2', 'hsa-mir-4330', 'hsa-mir-548a-1', 'hsa-mir-5787'])\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the csv\n",
    "test_data.to_csv('TEMP_Combined_Test_Data_4modald_mR_Gen_DMeth_Clin.csv', index=False)\n",
    "print('File saved as: TEMP_Combined_Test_Data_4modald_mR_Gen_DMeth_Clin.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the csv\n",
    "test_data = pd.read_csv('TEMP_Combined_Test_Data_4modald_mR_Gen_DMeth_Clin.csv')\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Get the Sample names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the sample names from test data\n",
    "samples = test_data['sample']\n",
    "print(samples.shape)\n",
    "# sort the samples\n",
    "samples = samples.sort_values()\n",
    "pd.set_option('display.max_rows', None)\n",
    "# reset the index\n",
    "samples = samples.reset_index(drop=True)\n",
    "print(samples)\n",
    "print(len(samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. The Protein Expression data for all cancer types is in a folder with patient_IDs as subdirectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the path of data folder\n",
    "path = 'TCGA-pancancer/data/raw'\n",
    "# list the number of folders in the data folder\n",
    "import os\n",
    "folders = os.listdir(path)\n",
    "folders.sort()\n",
    "print(\"List of Protein Expre data subfolders:\", folders)\n",
    "print(\"Total Patients:\", len(folders))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Match the Sample names from other features with the patient_IDs having Protein Expression data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the elements of samples and search through all subfolders of the path for afile starting with the sample name as samples*.tsv\n",
    "# initialize empty dataframe\n",
    "data = pd.DataFrame()\n",
    "# iterate through the samples\n",
    "for i in range(0, len(samples)):\n",
    "    # print(i+1, 'Sample: ', samples[i])\n",
    "    # iterate through the folders\n",
    "    for j in range(0, len(folders)):\n",
    "        # if string of sample except last 4 characters is the same as the folder name\n",
    "        if samples[i][:-4] == folders[j]:\n",
    "            print(j+1, 'Looking {} in folder {}'.format(samples[i], folders[j]))\n",
    "            # get the list of files in the folder that has this structure: '{path}/{folders[j]}/'Protein Expression Quantification'/*'\n",
    "            file = glob.glob(f'{path}/{folders[j]}/Protein Expression Quantification/*/{samples[i]}*')\n",
    "            # read the file.tsv if it exists\n",
    "            if len(file) > 0:\n",
    "                print(file)\n",
    "                # read the file columns having names 'AGID' and 'mutation'\n",
    "                df = pd.read_csv(file[0], sep='\\t', usecols=['AGID', 'protein_expression'])\n",
    "                print('df shape: ', df.shape)\n",
    "                # transpose the dataframe set AGID column as the column names and protein_expression as the values\n",
    "                df = df.transpose()\n",
    "                df = df.reset_index()\n",
    "                # set the columns to the first row\n",
    "                df.columns = df.iloc[0]\n",
    "                # remove the first row\n",
    "                df = df[1:]\n",
    "                # rename the first column to 'sample'\n",
    "                df = df.rename(columns={f'{df.columns[0]}':'sample'})\n",
    "                # set the sample name as the sample[i]\n",
    "                df['sample'] = samples[i]\n",
    "                # concatenate the df to data dataframe\n",
    "                data = pd.concat([data, df])\n",
    "                # reset the index\n",
    "                data = data.reset_index(drop=True)\n",
    "                print('data shape: ', data.shape)\n",
    "            else:\n",
    "                print('File not found in folder:', folders[j])\n",
    "#save the data to a csv file\n",
    "data.to_csv('Protein_Expression_Test_Data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Impute the NANs with the mean of the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the second column onwards, impute the NANs with the mean of the column\n",
    "data.iloc[:, 1:] = data.iloc[:, 1:].apply(lambda x: x.fillna(x.mean()), axis=0)\n",
    "# check if there are any NaN values\n",
    "print(data.isnull().sum().sum())\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the data to a csv file\n",
    "data.to_csv('Protein_Expression_Test_Data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Drop Constant Features (i.e. >99.8% similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_expr_test = data\n",
    "print(protein_expr_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine.selection import DropConstantFeatures\n",
    "sel1 = DropConstantFeatures(tol=0.998, variables=None, missing_values='raise')\n",
    "sel1.fit(protein_expr_test)\n",
    "protein_expr_test = sel1.transform(protein_expr_test)\n",
    "protein_expr_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Remove Colinear Features (only those removed from the training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temporary remove the first column for later adding it to the first column\n",
    "protein_expr_test1 = protein_expr_test.iloc[:, 1:]\n",
    "# check the variable format with pandas dtypes.\n",
    "print(protein_expr_test1.dtypes)\n",
    "# convert the variable to numerical variables\n",
    "protein_expr_test1 = protein_expr_test1.astype(float)\n",
    "# check the variable format with pandas dtypes.\n",
    "print(protein_expr_test1.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(protein_expr_test1.shape)\n",
    "print(protexpr.shape)\n",
    "# Keep the columns in protein_expr_test1 that are in protexpr\n",
    "protein_expr_test1 = protein_expr_test1[protexpr.columns]\n",
    "print(protein_expr_test1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify that the columns in protein_expr_test1 are the same as in protexpr\n",
    "if protein_expr_test1.columns.equals(protexpr.columns):\n",
    "    print(\"The columns in protein_expr_test1 and protexpr are the same.\")\n",
    "else:\n",
    "    print(\"The columns in protein_expr_test1 and protexpr are not the same.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Add OS labels and save the Protein Expression data as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the 'sample' column from protein_expr_test to the first column of protein_expr_test1\n",
    "protein_expr_test1.insert(0, 'sample', protein_expr_test['sample'])\n",
    "# add OS and OS.time columns from test_data to the protein_expr_test1 dataframe based on the sample name\n",
    "protein_expr_test1 = pd.merge(protein_expr_test1, test_data[['sample', 'OS', 'OS.time']], on='sample', how='inner')\n",
    "# Move the OS, OS.time to second and third columns of the dataframe\n",
    "cols = list(protein_expr_test1.columns)\n",
    "cols = [cols[0]] + [cols[-1]] + [cols[-2]] + cols[1:-2]\n",
    "protein_expr_test1 = protein_expr_test1[cols]\n",
    "protein_expr_test = protein_expr_test1\n",
    "print(protein_expr_test.shape)\n",
    "protein_expr_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_expr_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the data to a csv file\n",
    "protein_expr_test.to_csv('Protein_Expression_Test_Data_processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# laod the csv\n",
    "protein_expr_test = pd.read_csv('Protein_Expression_Test_Data_processed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the distribution of the OS.time variable\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set(style=\"whitegrid\")\n",
    "# plot distribution of OS.time\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(test_data['OS.time'], kde=True)\n",
    "plt.title('Distribution of Test OS.time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the distribution of the first 4 variables\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(test_data[test_data.columns[3]], kde=True)\n",
    "sns.histplot(test_data[test_data.columns[4]], kde=True)\n",
    "sns.histplot(test_data[test_data.columns[5]], kde=True)\n",
    "sns.histplot(test_data[test_data.columns[6]], kde=True)\n",
    "plt.title('Distribution of ' + test_data.columns[3] + ', ' + test_data.columns[4] + ', ' + test_data.columns[5] + ', ' + test_data.columns[6])\n",
    "plt.legend([test_data.columns[3], test_data.columns[4], test_data.columns[5], test_data.columns[6]])\n",
    "plt.xlabel('Expression')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the test_data.head() for 3rd columns onwards\n",
    "test_data.iloc[:, 7:].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data with zero mean and unit variance across the samples\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# initialize the standard scaler\n",
    "scaler = StandardScaler()\n",
    "# fit and transform the data without the first three columns\n",
    "test_data.iloc[:, 7:] = scaler.fit_transform(test_data.iloc[:, 7:])\n",
    "print('test shape:', test_data.shape)\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the distribution of the first 4 variables\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(test_data[test_data.columns[7]], kde=True)\n",
    "sns.histplot(test_data[test_data.columns[8]], kde=True)\n",
    "sns.histplot(test_data[test_data.columns[9]], kde=True)\n",
    "sns.histplot(test_data[test_data.columns[10]], kde=True)\n",
    "plt.title('Distribution of ' + test_data.columns[3] + ', ' + test_data.columns[4] + ', ' + test_data.columns[5] + ', ' + test_data.columns[6])\n",
    "plt.legend([test_data.columns[7], test_data.columns[8], test_data.columns[9], test_data.columns[10]])\n",
    "plt.xlabel('Expression')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Add the Protein Expression data to the combined data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the protein_expr_test and test_data dataframes using the 'sample', OS.time, and OS columns as the key but keep samples from both dataframes\n",
    "print(\"Protein Expression Data shape: \", protein_expr_test.shape)\n",
    "print(\"Train Data shape: \", test_data.shape)\n",
    "# merge the dataframes\n",
    "protexpr_plus_test_data = pd.merge(protein_expr_test, test_data, on=['sample', 'OS.time', 'OS'], how='outer')\n",
    "print(\"Combined Train Data shape: \", protexpr_plus_test_data.shape)\n",
    "# replace the NaN values with 0\n",
    "protexpr_plus_test_data = protexpr_plus_test_data.fillna(0)\n",
    "print(protexpr_plus_test_data.shape)\n",
    "protexpr_plus_test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. Save the combined data to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv\n",
    "protexpr_plus_test_data.to_csv('5Modal_Test_Data_mR_Gen_DMeth_Clin_Prot.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(protexpr_plus_test_data.shape)\n",
    "protexpr_plus_test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the columns having names 'age', 'gender', 'race', 'tumor_stage'\n",
    "# Get the current column names as a list\n",
    "cols = list(protexpr_plus_test_data.columns)\n",
    "# Define the columns to move and their new positions\n",
    "cols_to_move = ['age', 'gender', 'race', 'tumor_stage']\n",
    "new_positions = [3, 4, 5, 6]\n",
    "# Remove the columns to move from the current column list\n",
    "for col in cols_to_move:\n",
    "    cols.remove(col)\n",
    "# Insert the columns to move at their new positions\n",
    "for col, pos in zip(cols_to_move, new_positions):\n",
    "    cols.insert(pos, col)\n",
    "# Reindex the DataFrame with the new column order\n",
    "protexpr_plus_test_data_tmp = protexpr_plus_test_data[cols]\n",
    "protexpr_plus_test_data_tmp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(protexpr_plus_test_data_tmp.shape)\n",
    "fiveMod_Test_mR_Gen_DMeth_Clin_Prot = protexpr_plus_test_data_tmp\n",
    "print(fiveMod_Test_mR_Gen_DMeth_Clin_Prot.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. Sanity Check: verify that there are no zero-valued rows or columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if there are any rows that have all Zero values\n",
    "print(fiveMod_Test_mR_Gen_DMeth_Clin_Prot[(fiveMod_Test_mR_Gen_DMeth_Clin_Prot.iloc[:, 7:] == 0).all(axis=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if there are any columns that have all Zero values\n",
    "print(fiveMod_Test_mR_Gen_DMeth_Clin_Prot.columns[(fiveMod_Test_mR_Gen_DMeth_Clin_Prot == 0).all()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the fiveMod_Test_mR_Gen_DMeth_Clin_Prot to csv\n",
    "fiveMod_Test_mR_Gen_DMeth_Clin_Prot.to_csv('5Modal_Test_Data_mR_Gen_DMeth_Clin_Prot.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the csv\n",
    "fiveMod_Test_mR_Gen_DMeth_Clin_Prot = pd.read_csv('5Modal_Test_Data_mR_Gen_DMeth_Clin_Prot.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fiveMod_Test_mR_Gen_DMeth_Clin_Prot.shape)\n",
    "fiveMod_Test_mR_Gen_DMeth_Clin_Prot.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15. k-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the patient names from sample column\n",
    "test_patientnames=fiveMod_Test_mR_Gen_DMeth_Clin_Prot['sample']\n",
    "print(len(test_patientnames))\n",
    "print(test_patientnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate k-fold cross validation splits for the data, k=10\n",
    "from sklearn.model_selection import KFold\n",
    "# initialize the kfold object\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "test_folds = list(kf.split(test_patientnames))\n",
    "#save as .csv file having rows as patient names and columns as folds\n",
    "test_folds_array = np.zeros((len(test_patientnames), 10))\n",
    "for i in range(10):\n",
    "    test_folds_array[test_folds[i][1], i] = 1\n",
    "test_folds_df = pd.DataFrame(test_folds_array, columns=['fold_{}'.format(i) for i in range(1,11)])\n",
    "test_folds_df.index = test_patientnames\n",
    "#replace 0 with Train and 1 with Test\n",
    "test_folds_df = test_folds_df.replace(0, 'Train')\n",
    "test_folds_df = test_folds_df.replace(1, 'Test')\n",
    "test_folds_df.to_csv('test/pnas_fiveMod_Test_mR_Gen_DMeth_Clin_Prot_splits.csv')\n",
    "test_folds_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16. Save the Test Data in a .pkl file for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the samples, vital_status (OS) and labels (OS.time) from the dataframe and assign to numpy.ndarray\n",
    "# save the samples, vital_status (OS) and labels (OS.time) to numpy.ndarray\n",
    "samples = fiveMod_Test_mR_Gen_DMeth_Clin_Prot['sample'].values\n",
    "vital_status = fiveMod_Test_mR_Gen_DMeth_Clin_Prot['OS'].values\n",
    "survival = fiveMod_Test_mR_Gen_DMeth_Clin_Prot['OS.time'].values\n",
    "print('samples:', samples, 'vital_status:', vital_status, 'labels:', survival)\n",
    "print('samples shape:', samples.shape, 'vital_status shape:', vital_status.shape, 'survival shape:', survival.shape)\n",
    "# convert survival to df\n",
    "survival_df = pd.DataFrame(survival, columns=['Labels'])\n",
    "survival_df.index = samples\n",
    "survival_df = survival_df.astype(float)\n",
    "# convert vital_status to df\n",
    "vital_status_df = pd.DataFrame(vital_status, columns=['Vital_status'])\n",
    "vital_status_df.index = samples\n",
    "vital_status_df = vital_status_df.astype(float)\n",
    "# remove the vital_status (OS) and labels (OS.time) from the dataframe\n",
    "print('train shape:', fiveMod_Test_mR_Gen_DMeth_Clin_Prot.shape)\n",
    "omic = fiveMod_Test_mR_Gen_DMeth_Clin_Prot.drop(columns=['sample', 'OS', 'OS.time'])\n",
    "omic.index = samples\n",
    "print('omic shape:', omic.shape)\n",
    "omic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cv = {}\n",
    "data_cv['cv_splits'] = {}\n",
    "\n",
    "for i in range(10):\n",
    "    # add and store data for each fold in the data_cv dictionary\n",
    "    data_cv['cv_splits'][i+1] = {}\n",
    "    data_cv['cv_splits'][i+1]['train'] = {}\n",
    "    data_cv['cv_splits'][i+1]['test'] = {}\n",
    "    data_cv['cv_splits'][i+1]['train'] = {\n",
    "        'x_patname': test_folds_df[test_folds_df['fold_{}'.format(i+1)] == 'Train'].index.values.tolist(),\n",
    "        'x_omic': omic.loc[test_folds_df[test_folds_df['fold_{}'.format(i+1)] == 'Train'].index.values.tolist()].values,\n",
    "        'e': np.squeeze(vital_status_df.loc[test_folds_df[test_folds_df['fold_{}'.format(i+1)] == 'Train'].index.values.tolist()].values),\n",
    "        't': np.squeeze(survival_df.loc[test_folds_df[test_folds_df['fold_{}'.format(i+1)] == 'Train'].index.values.tolist()].values)\n",
    "    }\n",
    "    data_cv['cv_splits'][i+1]['test'] = {\n",
    "        'x_patname': test_folds_df[test_folds_df['fold_{}'.format(i+1)] == 'Test'].index.values.tolist(),\n",
    "        'x_omic': omic.loc[test_folds_df[test_folds_df['fold_{}'.format(i+1)] == 'Test'].index.values.tolist()].values,\n",
    "        'e': np.squeeze(vital_status_df.loc[test_folds_df[test_folds_df['fold_{}'.format(i+1)] == 'Test'].index.values.tolist()].values),\n",
    "        't': np.squeeze(survival_df.loc[test_folds_df[test_folds_df['fold_{}'.format(i+1)] == 'Test'].index.values.tolist()].values)\n",
    "    }\n",
    "\n",
    "# save the dictionary to a .pkl file\n",
    "with open('test/omic_test_10cv_all.pkl', 'wb') as f:\n",
    "    pickle.dump(data_cv, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERIFY THE PKL FILE\n",
    "#load the .pkl file\n",
    "import pickle\n",
    "data_cv = pickle.load(open('test/omic_test_10cv_all.pkl', 'rb'))\n",
    "data_cv_splits = data_cv['cv_splits']\n",
    "\n",
    "for k, data in data_cv_splits.items():\n",
    "\tprint(\"*******************************************\")\n",
    "\tprint(\"************** SPLIT (%d/%d) **************\" % (k, len(data_cv_splits.items())))\n",
    "\tprint(\"*******************************************\")\n",
    "\tif k == 1:\n",
    "\t\tprint(data_cv_splits)\n",
    "\tprint(len(data_cv_splits[k]['train']['x_patname']), (data_cv_splits[k]['train']['e']).shape, (data_cv_splits[k]['train']['t']).shape, (data_cv_splits[k]['train']['x_omic']).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('data_cv.keys: ', data_cv.keys())\n",
    "print(\"data_cv['cv_splits'].keys: \", data_cv['cv_splits'].keys())\n",
    "print(\"data_cv['cv_splits'][1]: \", data_cv['cv_splits'][1].keys())\n",
    "print(\"data_cv['cv_splits'][1]['test']: \", data_cv['cv_splits'][1]['test'].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since the single file is ~14GB, we will divide the data for each fold and save to separate .pkl files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 11):\n",
    "    # create a new dictionary for each fold\n",
    "    data_dict = {\n",
    "        'cv_splits': {\n",
    "            i: {\n",
    "                'train': data['cv_splits'][i]['train'],\n",
    "                'test': data['cv_splits'][i]['test']\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # save the dictionary to a .pkl file\n",
    "    with open('test/omic_test_10cv_fold_{}.pkl'.format(i), 'wb') as f:\n",
    "        pickle.dump(data_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load .pkl file for each fold and verify the data lengths\n",
    "for k in range(1, 11):\n",
    "    data = pickle.load(open('test/omic_test_10cv_fold_{}.pkl'.format(k), 'rb'))\n",
    "    print(\"*******************************************\")\n",
    "    print(\"************** SPLIT (%d/%d) **************\" % (k, 10))\n",
    "    print(\"*******************************************\")\n",
    "    print('data.keys: ', data.keys())\n",
    "    print(\"data['cv_splits'].keys: \", data['cv_splits'].keys())\n",
    "    print(\"data['cv_splits'][1]: \", data['cv_splits'][k].keys())\n",
    "    print(\"data['cv_splits'][1]['train']: \", data['cv_splits'][k]['train'].keys())\n",
    "    print(\"data['cv_splits'][1]['test']: \", data['cv_splits'][k]['test'].keys())\n",
    "    print(len(data['cv_splits'][k]['train']['x_patname']),\n",
    "            (data['cv_splits'][k]['train']['e']).shape,\n",
    "            (data['cv_splits'][k]['train']['t']).shape,\n",
    "            (data['cv_splits'][k]['train']['x_omic']).shape\n",
    "    )\n",
    "    print(len(data['cv_splits'][k]['test']['x_patname']), (data['cv_splits'][k]['test']['e']).shape, (data['cv_splits'][k]['test']['t']).shape, (data['cv_splits'][k]['test']['x_omic']).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  DNA Mutation data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiveMod_Test_mR_Gen_DMeth_Clin_Prot.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Get the Sample names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the sample names from test data\n",
    "samples = fiveMod_Test_mR_Gen_DMeth_Clin_Prot['sample']\n",
    "print(samples.shape)\n",
    "# sort the samples\n",
    "samples = samples.sort_values()\n",
    "pd.set_option('display.max_rows', None)\n",
    "# reset the index\n",
    "samples = samples.reset_index(drop=True)\n",
    "print(samples)\n",
    "print(len(samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Load the DNA Mutation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load DNA Mut data from DNA_mut_data/DNA_union_df.csv\n",
    "DNA_mut = pd.read_csv('DNA_mut_data/DNA_union_df.csv')\n",
    "print('Opening file: DNA_mut_data/DNA_union_df.csv')\n",
    "print(DNA_mut.shape)\n",
    "DNA_mut.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Match the Sample names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(samples.shape)\n",
    "print(DNA_mut['SampleID'].shape)\n",
    "# get the DNA_mut data based on DNA_mut['SampleID']=samples[:-4]\n",
    "DNA_mut_test = DNA_mut[DNA_mut['SampleID'].isin(samples.str[:-4])]\n",
    "print(DNA_mut_test.shape)\n",
    "DNA_mut_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Impute the NANs with the mean of the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if there are any NANs for second column onwards\n",
    "print(DNA_mut_test.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Drop Constant Features (i.e. >100% similarity (all zeros))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if any column has all zero values\n",
    "print(DNA_mut_test.columns[(DNA_mut_test == 0).all()])\n",
    "# get all the column names having all zero values in alist\n",
    "all_zero_columns = DNA_mut_test.columns[(DNA_mut_test == 0).all()].tolist()\n",
    "print(len(all_zero_columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's keep the same feature size as in training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the train data\n",
    "# load file DNA_Mut_Train_Data_processed.csv\n",
    "DNAmuts_train = pd.read_csv('DNA_Mut_Train_Data_processed.csv')\n",
    "print('Opening file: DNA_Mut_Train_Data_processed.csv')\n",
    "print(DNAmuts_train.shape)\n",
    "DNAmuts_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop the features from test data that are not in training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DNA Mut Test data shape:\", DNA_mut_test.shape)\n",
    "print(\"DNA Mut Train data shape:\", DNAmuts_train.shape)\n",
    "# keep the SampleID column in the DNA_mut_test dataframe\n",
    "SampleIDs = DNA_mut_test['SampleID']\n",
    "# Drop the features from DNA_mut_test (except SampleID column) that are not in DNAmuts_train\n",
    "DNA_muts_test = DNA_mut_test.drop(columns=[col for col in DNA_mut_test.columns if col not in DNAmuts_train.columns])\n",
    "print(\"DNA Muts Test data shape:\", DNA_muts_test.shape)\n",
    "print(DNA_muts_test.head())\n",
    "# add the SampleID column to the DNA_muts_test dataframe\n",
    "DNA_muts_test.insert(0, 'SampleID', SampleIDs)\n",
    "print(\"DNA Muts Test data shape:\", DNA_muts_test.shape)\n",
    "print(DNA_muts_test.head())\n",
    "# reset the index\n",
    "DNA_muts_test = DNA_muts_test.reset_index(drop=True)\n",
    "DNA_muts_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(DNA_muts_test.shape)\n",
    "DNA_muts_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Add OS labels and save the DNA Mut data as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add 'sample, 'OS', 'OS.time' from four_modal_test to DNA_muts_test\n",
    "# get the four_modal_test['sample', 'OS', 'OS.time'] in a separate df\n",
    "fivemodal_test_3 = fiveMod_Test_mR_Gen_DMeth_Clin_Prot[['sample', 'OS', 'OS.time']]\n",
    "print(fivemodal_test_3.shape)\n",
    "print(fivemodal_test_3.head())\n",
    "# add fivemodal_test_3['sample'].str[:-4] to fivemodal_test_3 as first column\n",
    "fivemodal_test_3.insert(0, 'SampleID', fivemodal_test_3['sample'].str[:-4])\n",
    "print(fivemodal_test_3.shape)\n",
    "print(fivemodal_test_3.head())\n",
    "# add fivemodal_test_3['sample] to DNA_muts_test as first column based on SampleID=sample[:-4]\n",
    "DNAMut_test_data = pd.merge(DNA_muts_test, fivemodal_test_3[['SampleID', 'sample', 'OS', 'OS.time']], left_on='SampleID', right_on='SampleID', how='inner')\n",
    "print(DNAMut_test_data.shape)\n",
    "# Move the sample, OS, OS.time to first, second and third columns of the dataframe\n",
    "cols = list(DNAMut_test_data.columns)\n",
    "cols = [cols[-3]] + [cols[-1]] + [cols[-2]] + cols[0:-3]\n",
    "DNAMut_test_data = DNAMut_test_data[cols]\n",
    "print(DNAMut_test_data.shape)\n",
    "DNAMut_test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the duplicate rows\n",
    "print(DNAMut_test_data['SampleID'].duplicated().sum())\n",
    "print(DNAMut_test_data['sample'].duplicated().sum())\n",
    "# get the duplicate rows based on DNAMut_data['SampleID']\n",
    "print(DNAMut_test_data[DNAMut_test_data['SampleID'].duplicated()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the duplicate rows based on sample\n",
    "print(DNAMut_test_data.shape)\n",
    "DM_test_data = DNAMut_test_data.drop_duplicates(subset='sample', keep='first')\n",
    "print(DM_test_data.shape)\n",
    "print(DM_test_data['SampleID'].duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DM_test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the SampleID column\n",
    "DM_test_data = DM_test_data.drop(columns='SampleID')\n",
    "print(DM_test_data.shape)\n",
    "DM_test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to a csv\n",
    "DM_test_data.to_csv('DNA_Mut_Test_Data_processed_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the csv and verify the dimensions of the saved file\n",
    "DM_test_data = pd.read_csv('DNA_Mut_Test_Data_processed_final.csv')\n",
    "print('Opening file: DNA_Mut_Test_Data_processed_final.csv')\n",
    "print(DM_test_data.shape)\n",
    "DM_test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Normalize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We don't need to normalize, because the DNA Mut data is already binary, and the 4Modality data is already normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Add the DNA Mut data to the combined data (four_modal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the DNA Mut and fiveMod_Test_mR_Gen_DMeth_Clin_Prot dataframes using the 'sample', OS.time, and OS columns as the key but keep samples from both dataframes\n",
    "print(\"DNA Mut Data shape: \", DM_test_data.shape)\n",
    "print(\"5Modal Test Data shape: \", fiveMod_Test_mR_Gen_DMeth_Clin_Prot.shape)\n",
    "# merge the dataframes\n",
    "sixmodal_test_data = pd.merge(DM_test_data, fiveMod_Test_mR_Gen_DMeth_Clin_Prot, on=['sample', 'OS.time', 'OS'], how='outer')\n",
    "print(\"Combined Test Data shape: \", sixmodal_test_data.shape)\n",
    "# replace the NaN values with 0\n",
    "sixmodal_test_data = sixmodal_test_data.fillna(0)\n",
    "print(sixmodal_test_data.shape)\n",
    "sixmodal_test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the columns having names 'age', 'gender', 'race', 'tumor_stage'\n",
    "# Get the current column names as a list\n",
    "cols = list(sixmodal_test_data.columns)\n",
    "# Define the columns to move and their new positions\n",
    "cols_to_move = ['age', 'gender', 'race', 'tumor_stage']\n",
    "new_positions = [3, 4, 5, 6]\n",
    "# Remove the columns to move from the current column list\n",
    "for col in cols_to_move:\n",
    "    cols.remove(col)\n",
    "# Insert the columns to move at their new positions\n",
    "for col, pos in zip(cols_to_move, new_positions):\n",
    "    cols.insert(pos, col)\n",
    "# Reindex the DataFrame with the new column order\n",
    "sixmodal_test_data_tmp = sixmodal_test_data[cols]\n",
    "sixmodal_test_data_tmp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Save the combined data to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to a csv\n",
    "sixmodal_test_data_tmp.to_csv('6Modal_Test_Data_mR_Gen_DMeth_Clin_Prot_Mut.csv', index=False)\n",
    "print('saving to 6Modal_Test_Data_mR_Gen_DMeth_Clin_Prot_Mut.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. Sanity Check: verify that there are no zero-valued rows or columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sixmodal_test_data = sixmodal_test_data_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if there are any rows that have all Zero values\n",
    "# find the rows having all zero values\n",
    "print(sixmodal_test_data[(sixmodal_test_data.iloc[:, 3:] == 0).all(axis=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if there are any columns that have all Zero values\n",
    "# find the columns having all zero values\n",
    "print(sixmodal_test_data.columns[(sixmodal_test_data == 0).all()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. k-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the patient names from sample column\n",
    "test_patientnames=sixmodal_test_data['sample']\n",
    "print(len(test_patientnames))\n",
    "print(test_patientnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate k-fold cross validation splits for the data, k=10\n",
    "from sklearn.model_selection import KFold\n",
    "# initialize the kfold object\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "folds = list(kf.split(test_patientnames))\n",
    "#save as .csv file having rows as patient names and columns as folds\n",
    "folds_array = np.zeros((len(test_patientnames), 10))\n",
    "for i in range(10):\n",
    "    folds_array[folds[i][1], i] = 1\n",
    "folds_df = pd.DataFrame(folds_array, columns=['fold_{}'.format(i) for i in range(1,11)])\n",
    "folds_df.index = test_patientnames\n",
    "#replace 0 with Train and 1 with Test\n",
    "folds_df = folds_df.replace(0, 'Train')\n",
    "folds_df = folds_df.replace(1, 'Test')\n",
    "folds_df.to_csv('test/pnas_test_splits.csv')\n",
    "folds_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15. Save the Test Data in a .pkl file for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the samples, vital_status (OS) and labels (OS.time) to numpy.ndarray\n",
    "samples = sixmodal_test_data['sample'].values\n",
    "vital_status = sixmodal_test_data['OS'].values\n",
    "survival = sixmodal_test_data['OS.time'].values\n",
    "print('samples:', samples, 'vital_status:', vital_status, 'labels:', survival)\n",
    "print('samples shape:', samples.shape, 'vital_status shape:', vital_status.shape, 'survival shape:', survival.shape)\n",
    "# convert survival to df\n",
    "survival_df = pd.DataFrame(survival, columns=['Labels'])\n",
    "survival_df.index = samples\n",
    "survival_df = survival_df.astype(float)\n",
    "# convert vital_status to df\n",
    "vital_status_df = pd.DataFrame(vital_status, columns=['Vital_status'])\n",
    "vital_status_df.index = samples\n",
    "vital_status_df = vital_status_df.astype(float)\n",
    "# remove the vital_status (OS) and labels (OS.time) from the dataframe\n",
    "print('test shape:', sixmodal_test_data.shape)\n",
    "omic = sixmodal_test_data.drop(columns=['sample', 'OS', 'OS.time'])\n",
    "omic.index = samples\n",
    "print('omic shape:', omic.shape)\n",
    "omic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # convert dataframes to numpy arrays\n",
    "data_cv = {}\n",
    "data_cv['cv_splits'] = {}\n",
    "\n",
    "for i in range(10):\n",
    "    # add and store data for each fold in the data_cv dictionary\n",
    "    data_cv['cv_splits'][i+1] = {}\n",
    "    data_cv['cv_splits'][i+1]['train'] = {}\n",
    "    data_cv['cv_splits'][i+1]['test'] = {}\n",
    "    data_cv['cv_splits'][i+1]['train'] = {\n",
    "        'x_patname': folds_df[folds_df['fold_{}'.format(i+1)] == 'Train'].index.values.tolist(),\n",
    "        'x_omic': omic.loc[folds_df[folds_df['fold_{}'.format(i+1)] == 'Train'].index.values.tolist()].values,\n",
    "        'e': np.squeeze(vital_status_df.loc[folds_df[folds_df['fold_{}'.format(i+1)] == 'Train'].index.values.tolist()].values),\n",
    "        't': np.squeeze(survival_df.loc[folds_df[folds_df['fold_{}'.format(i+1)] == 'Train'].index.values.tolist()].values)\n",
    "    }\n",
    "    data_cv['cv_splits'][i+1]['test'] = {\n",
    "        'x_patname': folds_df[folds_df['fold_{}'.format(i+1)] == 'Test'].index.values.tolist(),\n",
    "        'x_omic': omic.loc[folds_df[folds_df['fold_{}'.format(i+1)] == 'Test'].index.values.tolist()].values,\n",
    "        'e': np.squeeze(vital_status_df.loc[folds_df[folds_df['fold_{}'.format(i+1)] == 'Test'].index.values.tolist()].values),\n",
    "        't': np.squeeze(survival_df.loc[folds_df[folds_df['fold_{}'.format(i+1)] == 'Test'].index.values.tolist()].values)\n",
    "    }\n",
    "\n",
    "# save the dictionary to a .pkl file\n",
    "with open('test/omic_test_10cv.pkl', 'wb') as f:\n",
    "    pickle.dump(data_cv, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERIFY THE PKL FILE\n",
    "#load the .pkl file\n",
    "import pickle\n",
    "data_cv = pickle.load(open('test/omic_test_10cv.pkl', 'rb'))\n",
    "data_cv_splits = data_cv['cv_splits']\n",
    "\n",
    "for k, data in data_cv_splits.items():\n",
    "\tprint(\"*******************************************\")\n",
    "\tprint(\"************** SPLIT (%d/%d) **************\" % (k, len(data_cv_splits.items())))\n",
    "\tprint(\"*******************************************\")\n",
    "\tif k == 1:\n",
    "\t\tprint(data_cv_splits)\n",
    "\tprint(len(data_cv_splits[k]['test']['x_patname']), (data_cv_splits[k]['test']['e']).shape, (data_cv_splits[k]['test']['t']).shape, (data_cv_splits[k]['test']['x_omic']).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('data_cv.keys: ', data_cv.keys())\n",
    "print(\"data_cv['cv_splits'].keys: \", data_cv['cv_splits'].keys())\n",
    "print(\"data_cv['cv_splits'][1]: \", data_cv['cv_splits'][1].keys())\n",
    "print(\"data_cv['cv_splits'][1]['train']: \", data_cv['cv_splits'][1]['train'].keys())\n",
    "print(\"data_cv['cv_splits'][1]['test']: \", data_cv['cv_splits'][1]['test'].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since the single file is ~17GB, we will divide the data for each fold and save to separate .pkl files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 11):\n",
    "    # create a new dictionary for each fold\n",
    "    data_dict = {\n",
    "        'cv_splits': {\n",
    "            i: {\n",
    "                'train': data['cv_splits'][i]['train'],\n",
    "                'test': data['cv_splits'][i]['test']\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # save the dictionary to a .pkl file\n",
    "    with open('test/omic_test_10cv_fold_{}.pkl'.format(i), 'wb') as f:\n",
    "        pickle.dump(data_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load .pkl file for each fold and verify the data lengths\n",
    "for k in range(1, 11):\n",
    "    data = pickle.load(open('test/omic_test_10cv_fold_{}.pkl'.format(k), 'rb'))\n",
    "    print(\"*******************************************\")\n",
    "    print(\"************** SPLIT (%d/%d) **************\" % (k, 10))\n",
    "    print(\"*******************************************\")\n",
    "    print('data.keys: ', data.keys())\n",
    "    print(\"data['cv_splits'].keys: \", data['cv_splits'].keys())\n",
    "    print(\"data['cv_splits'][1]: \", data['cv_splits'][k].keys())\n",
    "    print(\"data['cv_splits'][1]['train']: \", data['cv_splits'][k]['train'].keys())\n",
    "    print(\"data['cv_splits'][1]['test']: \", data['cv_splits'][k]['test'].keys())\n",
    "    print(len(data['cv_splits'][k]['train']['x_patname']),\n",
    "            (data['cv_splits'][k]['train']['e']).shape,\n",
    "            (data['cv_splits'][k]['train']['t']).shape,\n",
    "            (data['cv_splits'][k]['train']['x_omic']).shape\n",
    "    )\n",
    "    print(len(data['cv_splits'][k]['test']['x_patname']), (data['cv_splits'][k]['test']['e']).shape, (data['cv_splits'][k]['test']['t']).shape, (data['cv_splits'][k]['test']['x_omic']).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's combine all folds into one pkl for the test samples only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the .pkl file omic_test_10cv.pkl\n",
    "import pickle\n",
    "data = pickle.load(open('test/omic_test_10cv.pkl', 'rb'))\n",
    "print('opening file: test/omic_test_10cv.pkl')\n",
    "print('data.keys: ', data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(1, 11):\n",
    "    print(len(data['cv_splits'][k]['test']['x_patname']), (data['cv_splits'][k]['test']['e']).shape, (data['cv_splits'][k]['test']['t']).shape, (data['cv_splits'][k]['test']['x_omic']).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_test_data = {\n",
    "            'cv_splits': {\n",
    "                1: {\n",
    "                    'test': {'x_patname': [], 'x_omic': [], 'e': [], 't': []} \n",
    "                }\n",
    "            }\n",
    "        }\n",
    "print(combined_test_data)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "for key in data['cv_splits'].keys():\n",
    "    print('key: ', key)\n",
    "    for subkey in data['cv_splits'][key]['test'].keys():\n",
    "        # print('subkey :', subkey)\n",
    "        subdata = data['cv_splits'][key]['test'][subkey]\n",
    "        if np.isscalar(subdata) or np.ndim(subdata) == 0:\n",
    "            subdata = [subdata]\n",
    "        combined_test_data['cv_splits'][1]['test'][subkey].extend(subdata)\n",
    "\n",
    "# Save combined data into a new .pkl file\n",
    "with open('test/omic_test_combined.pkl', 'wb') as f:\n",
    "    print('saving as test/omic_test_combined.pkl')\n",
    "    pickle.dump(combined_test_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved pkl file\n",
    "data_test = pickle.load(open('test/omic_test_combined.pkl', 'rb'))\n",
    "print('opening file: test/omic_test_combined.pkl')\n",
    "print('data.keys: ', data_test.keys())\n",
    "print(\"data_cv['cv_splits'].keys: \", data_test['cv_splits'].keys())\n",
    "print(\"data_cv['cv_splits'][1]: \", data_test['cv_splits'][1].keys())\n",
    "print(\"data_cv['cv_splits'][1]['test']: \", data_test['cv_splits'][1]['test'].keys())\n",
    "print(\"data_cv['cv_splits'][1]['test']['x_patname']: \", len(data_test['cv_splits'][1]['test']['x_patname']))\n",
    "print(\"data_cv['cv_splits'][1]['test']['x_omic']: \", len(data_test['cv_splits'][1]['test']['x_omic']))\n",
    "print(\"data_cv['cv_splits'][1]['test']['e']: \", len(data_test['cv_splits'][1]['test']['e']))\n",
    "print(\"data_cv['cv_splits'][1]['test']['t']: \", len(data_test['cv_splits'][1]['test']['t']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ----------- End of Inference Data Preprocessing -----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data for generating embeddings...combine train & test cohorts in one place"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Train csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the csv file\n",
    "sixmodal_train_data = pd.read_csv('6Modal_Train_Data_mR_Gen_DMeth_Clin_Prot_Mut.csv')\n",
    "print('Opening file: 6Modal_Train_Data_mR_Gen_DMeth_Clin_Prot_Mut.csv')\n",
    "print(sixmodal_train_data.shape)\n",
    "sixmodal_train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sixmodal_train_data[(sixmodal_train_data.iloc[:, 3:] == 0).all(axis=1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Test csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sixmodal_test_data = pd.read_csv('6Modal_Test_Data_mR_Gen_DMeth_Clin_Prot_Mut.csv')\n",
    "print('Opening file: 6Modal_Test_Data_mR_Gen_DMeth_Clin_Prot_Mut.csv')\n",
    "print(sixmodal_test_data.shape)\n",
    "sixmodal_test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sixmodal_test_data[(sixmodal_test_data.iloc[:, 3:] == 0).all(axis=1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine the Training and Testing dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine sixmodal_train_data and sixmodal_test_data\n",
    "combined_data = pd.concat([sixmodal_train_data, sixmodal_test_data], ignore_index=True)\n",
    "print(combined_data.shape)\n",
    "combined_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide in 2 folds temporarily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the patient names from sample column\n",
    "combined_patientnames=combined_data['sample']\n",
    "print(len(combined_patientnames))\n",
    "print(combined_patientnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate k-fold cross validation splits for the data, k=10\n",
    "from sklearn.model_selection import KFold\n",
    "# initialize the kfold object\n",
    "kf = KFold(n_splits=2, shuffle=True, random_state=42)\n",
    "folds = list(kf.split(combined_patientnames))\n",
    "#save as .csv file having rows as patient names and columns as folds\n",
    "folds_array = np.zeros((len(combined_patientnames), 2))\n",
    "for i in range(2):\n",
    "    folds_array[folds[i][1], i] = 1\n",
    "folds_df = pd.DataFrame(folds_array, columns=['fold_{}'.format(i) for i in range(1,3)])\n",
    "folds_df.index = combined_patientnames\n",
    "#replace 0 with Train and 1 with Test\n",
    "folds_df = folds_df.replace(0, 'Train')\n",
    "folds_df = folds_df.replace(1, 'Test')\n",
    "folds_df.to_csv('pancancer_combined/data_splits.csv')\n",
    "folds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the samples, vital_status (OS) and labels (OS.time) to numpy.ndarray\n",
    "samples = combined_data['sample'].values\n",
    "vital_status = combined_data['OS'].values\n",
    "survival = combined_data['OS.time'].values\n",
    "print('samples:', samples, 'vital_status:', vital_status, 'labels:', survival)\n",
    "print('samples shape:', samples.shape, 'vital_status shape:', vital_status.shape, 'survival shape:', survival.shape)\n",
    "# convert survival to df\n",
    "survival_df = pd.DataFrame(survival, columns=['Labels'])\n",
    "survival_df.index = samples\n",
    "survival_df = survival_df.astype(float)\n",
    "# convert vital_status to df\n",
    "vital_status_df = pd.DataFrame(vital_status, columns=['Vital_status'])\n",
    "vital_status_df.index = samples\n",
    "vital_status_df = vital_status_df.astype(float)\n",
    "# remove the vital_status (OS) and labels (OS.time) from the dataframe\n",
    "print('test shape:', combined_data.shape)\n",
    "omic = combined_data.drop(columns=['sample', 'OS', 'OS.time'])\n",
    "omic.index = samples\n",
    "print('omic shape:', omic.shape)\n",
    "omic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # convert dataframes to numpy arrays\n",
    "data_cv = {}\n",
    "data_cv['cv_splits'] = {}\n",
    "\n",
    "for i in range(2):\n",
    "    # add and store data for each fold in the data_cv dictionary\n",
    "    data_cv['cv_splits'][i+1] = {}\n",
    "    data_cv['cv_splits'][i+1]['test'] = {}\n",
    "    data_cv['cv_splits'][i+1]['test'] = {\n",
    "        'x_patname': folds_df[folds_df['fold_{}'.format(i+1)] == 'Test'].index.values.tolist(),\n",
    "        'x_omic': omic.loc[folds_df[folds_df['fold_{}'.format(i+1)] == 'Test'].index.values.tolist()].values,\n",
    "        'e': np.squeeze(vital_status_df.loc[folds_df[folds_df['fold_{}'.format(i+1)] == 'Test'].index.values.tolist()].values),\n",
    "        't': np.squeeze(survival_df.loc[folds_df[folds_df['fold_{}'.format(i+1)] == 'Test'].index.values.tolist()].values)\n",
    "    }\n",
    "\n",
    "# save the dictionary to a .pkl file\n",
    "with open('pancancer_combined/omics_2cv.pkl', 'wb') as f:\n",
    "    pickle.dump(data_cv, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERIFY THE PKL FILE\n",
    "data_cv = pickle.load(open('pancancer_combined/omics_2cv.pkl', 'rb'))\n",
    "data_cv_splits = data_cv['cv_splits']\n",
    "\n",
    "for k, data in data_cv_splits.items():\n",
    "\tprint(\"*******************************************\")\n",
    "\tprint(\"************** SPLIT (%d/%d) **************\" % (k, len(data_cv_splits.items())))\n",
    "\tprint(\"*******************************************\")\n",
    "\tif k == 1:\n",
    "\t\tprint(data_cv_splits)\n",
    "\tprint(len(data_cv_splits[k]['test']['x_patname']), (data_cv_splits[k]['test']['e']).shape, (data_cv_splits[k]['test']['t']).shape, (data_cv_splits[k]['test']['x_omic']).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('data_cv.keys: ', data_cv.keys())\n",
    "print(\"data_cv['cv_splits'].keys: \", data_cv['cv_splits'].keys())\n",
    "print(\"data_cv['cv_splits'][1]: \", data_cv['cv_splits'][1].keys())\n",
    "print(\"data_cv['cv_splits'][1]['test']: \", data_cv['cv_splits'][1]['test'].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now combine the two folds pkl in one test pkl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('data.keys: ', data.keys())\n",
    "for k in range(1, 3):\n",
    "    print(len(data['cv_splits'][k]['test']['x_patname']), (data['cv_splits'][k]['test']['e']).shape, (data['cv_splits'][k]['test']['t']).shape, (data['cv_splits'][k]['test']['x_omic']).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_test_data = {\n",
    "            'cv_splits': {\n",
    "                1: {\n",
    "                    'test': {'x_patname': [], 'x_omic': [], 'e': [], 't': []} \n",
    "                }\n",
    "            }\n",
    "        }\n",
    "print(combined_test_data)\n",
    "for key in data['cv_splits'].keys():\n",
    "    print('key: ', key)\n",
    "    for subkey in data['cv_splits'][key]['test'].keys():\n",
    "        # print('subkey :', subkey)\n",
    "        subdata = data['cv_splits'][key]['test'][subkey]\n",
    "        if np.isscalar(subdata) or np.ndim(subdata) == 0:\n",
    "            subdata = [subdata]\n",
    "        combined_test_data['cv_splits'][1]['test'][subkey].extend(subdata)\n",
    "\n",
    "# Save combined data into a new .pkl file\n",
    "with open('pancancer_combined/omic_combined.pkl', 'wb') as f:\n",
    "    print('saving as pancancer_combined/omic_combined.pkl')\n",
    "    pickle.dump(combined_test_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify the number of samples in the saved data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved pkl file\n",
    "data_test = pickle.load(open('pancancer_combined/omic_combined.pkl', 'rb'))\n",
    "print('opening file: pancancer_combined/omic_combined.pkl')\n",
    "print('data.keys: ', data_test.keys())\n",
    "print(\"data_cv['cv_splits'].keys: \", data_test['cv_splits'].keys())\n",
    "print(\"data_cv['cv_splits'][1]: \", data_test['cv_splits'][1].keys())\n",
    "print(\"data_cv['cv_splits'][1]['test']: \", data_test['cv_splits'][1]['test'].keys())\n",
    "print(\"data_cv['cv_splits'][1]['test']['x_patname']: \", len(data_test['cv_splits'][1]['test']['x_patname']))\n",
    "print(\"data_cv['cv_splits'][1]['test']['x_omic']: \", len(data_test['cv_splits'][1]['test']['x_omic']))\n",
    "print(\"data_cv['cv_splits'][1]['test']['e']: \", len(data_test['cv_splits'][1]['test']['e']))\n",
    "print(\"data_cv['cv_splits'][1]['test']['t']: \", len(data_test['cv_splits'][1]['test']['t']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert the Embeddings from pkl to parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pkl file having the features for the 33 cancers patients\n",
    "embdgs = pickle.load(open('pancancer_combined/FINAL_pancancer_combined_omic_embdgs.pkl', 'rb'))\n",
    "print('opening file: pancancer_combined/FINAL_pancancer_combined_omic_embdgs.pkl')\n",
    "print('number of patients:', len(embdgs))\n",
    "print('number of features:', len(embdgs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of patients and features\n",
    "num_patients = len(embdgs[0][0])\n",
    "num_features = len(embdgs[0][1])\n",
    "print(f\"Number of patients: {num_patients}\")\n",
    "print(f\"Number of features: {num_features}\")\n",
    "# Get the number of features for each patient\n",
    "features_per_patient = [len(patient_features) for patient_features in embdgs[0][1]]\n",
    "print(f\"Features per patient: {features_per_patient}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the patient name and corresponding feature size\n",
    "for patient, features in zip(embdgs[0][0], embdgs[0][1]):\n",
    "    print(f\"Patient: {patient}, Feature size: {len(features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with a column for PatientID and a second column with the embeddings\n",
    "df = pd.DataFrame({\n",
    "    'SampleID': embdgs[0][0],\n",
    "    'Embeddings': embdgs[0][1]\n",
    "})\n",
    "\n",
    "# Add a PatientID column as the first column\n",
    "df.insert(0, 'PatientID', df['SampleID'].str[:-4])\n",
    "\n",
    "# Save the DataFrame to a parquet file\n",
    "df.to_parquet(\"pancancer_combined/patients_embeddings.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the parquet file\n",
    "df = pd.read_parquet(\"pancancer_combined/patients_embeddings.parquet\")\n",
    "# Print the first few rows of the DataFrame\n",
    "print(df.head())\n",
    "# Print the shape of the DataFrame\n",
    "print(f\"Shape: {df.shape}\")\n",
    "# Print the column names\n",
    "print(f\"Columns: {df.columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TCGGAenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
